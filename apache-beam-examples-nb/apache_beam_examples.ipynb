{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19f571ff-1bc7-4912-bd38-9410885ad7a2",
   "metadata": {},
   "source": [
    "https://play.beam.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ace0d5-fa31-4af1-b011-4f60f3ecadf0",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed1476f3-7923-4c42-9714-8ffe367dc6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3acaca7-92ff-48ae-bebe-77ca9c5d9d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f34bd205-f61e-429e-9273-be026257bebd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as p:\n",
    "\n",
    "  (p | beam.Create(range(0, 11,2)[1:])\n",
    "     | beam.combiners.Mean.Globally()\n",
    "     | beam.io.textio.WriteToText('example-output')\n",
    "    )\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "020c6b5b-4bbd-4761-b677-afd875273e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "  (p | beam.Create(range(1, 11))\n",
    "     | beam.Filter(lambda num: num % 2 == 0)\n",
    "     | 'Write' >> beam.io.textio.WriteToText('example-output'))\n",
    "    \n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a2288f58-1656-4155-bff3-e8f9f85d2026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "  (p | beam.Create(range(1, 11))\n",
    "     | beam.Map(lambda num: num * 2)\n",
    "     | 'Write' >> beam.io.textio.WriteToText('example-output'))\n",
    "    \n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "75d04c62-875a-4975-a2f2-d884b80a3ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "class FilterOutEvenNumber(beam.DoFn):\n",
    "\n",
    "    def process(self, element):\n",
    "        if element % 2 == 0:\n",
    "            yield element\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "  (p | beam.Create(range(1, 11))\n",
    "     | beam.ParDo(FilterOutEvenNumber())\n",
    "    | 'Write' >> beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e7d9438-7fdc-4690-a8f9-f34a71b17d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ceefaf7-264e-4d27-b96c-29d1812dbd4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 2, 4, 6, 8, 10], 5.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0, 11,2)),np.mean(list(range(0, 11,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f73e6142-c70a-4c1d-9c9e-3acce2fa101c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 9, 8, 7]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "  (p | beam.Create(range(1, 11))\n",
    "     | beam.combiners.Top.Largest(4)\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "925b4add-8dab-4fd8-9f2f-dac2ed8b8462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "  (p | beam.Create(range(1, 11))\n",
    "     | beam.combiners.Top.Smallest(2)\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cd620f7-4508-4228-a85f-968e53448929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "  (p | beam.Create(range(1, 11))\n",
    "     | beam.CombineGlobally(min)\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc0effb3-380d-4039-8e85-e2daf51e4278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputtext.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile inputtext.txt\n",
    "10\n",
    "20\n",
    "30\n",
    "40\n",
    "100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d72280b3-e78e-49f0-8214-3c6d8c9b559e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "45\n",
      "65\n",
      "85\n",
      "205\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    # | beam.Create([10, 20, 30, 40, 50])\n",
    "  (p | beam.io.ReadFromText('inputtext.txt')\n",
    "     | beam.Map(lambda num: 2*int(num) + 5)\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "593ded3e-453e-456a-83dc-5efec4b330e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputtext2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile inputtext2.txt\n",
    "10,20,30\n",
    "20,30,35\n",
    "30,35,40\n",
    "40,45,50\n",
    "100,105,110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa61d5cf-97f5-4eb5-b918-b4f86720b0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 45, 65]\n",
      "[45, 65, 75]\n",
      "[65, 75, 85]\n",
      "[85, 95, 105]\n",
      "[205, 215, 225]\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    # | beam.Create([10, 20, 30, 40, 50])\n",
    "  (p | beam.io.ReadFromText('inputtext2.txt')\n",
    "     | beam.Map(lambda line: line.split(\",\"))\n",
    "     | beam.Map(lambda nums: [2*int(num) + 5 for num in nums])\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae38cc2c-6329-44c2-9539-3a31c31cfd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputtext3.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile inputtext3.txt\n",
    "10,20,30,B,C\n",
    "20,30,35,A,D\n",
    "30,35,40,K,E\n",
    "40,45,50,N,C\n",
    "100,105,110,M,C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f8a5e9c-6e07-46b0-a848-31192501c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strint(line):\n",
    "    col=[]\n",
    "    for x in line:\n",
    "        try:\n",
    "            col.append(int(x))\n",
    "        except:\n",
    "            col.append(x)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a22eac6-5f73-43e9-8ec3-12f2cf56e50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 20, 30, 'B', 'C']\n",
      "[20, 30, 35, 'A', 'D']\n",
      "[30, 35, 40, 'K', 'E']\n",
      "[40, 45, 50, 'N', 'C']\n",
      "[100, 105, 110, 'M', 'C']\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    # | beam.Create([10, 20, 30, 40, 50])\n",
    "  (p | beam.io.ReadFromText('inputtext3.txt')\n",
    "     | beam.Map(lambda line : line.split(\",\"))\n",
    "     | beam.Map(lambda num : strint(num))\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e78eff5-eba4-4268-9ea9-97371dd57212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e9f2a9b-a9c4-4395-b85b-800bc15715f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3818d21b-427a-473f-a974-8d88983d3caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 12, 22, 8, 15, 37, 677568)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.fromtimestamp(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62f3d9f0-67b6-425c-9582-a0ce9cc395b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 12, 21, 17, 57, 43)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.strptime(\"22-12-21 17:57:43\",\"%y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "340336d1-f904-4583-884e-1812e1a24006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputtext4.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile inputtext4.txt\n",
    "10,20,30,B,C,22-12-21 17:57:43\n",
    "20,30,35,A,D,22-12-21 17:57:43\n",
    "30,35,40,K,E,22-12-21 17:57:43\n",
    "40,45,50,N,C,22-12-21 17:57:43\n",
    "100,105,110,M,C,22-12-21 17:57:43\n",
    "10,20,30,B,C,22-12-21 17:57:43\n",
    "20,30,35,A,D,22-12-21 17:57:43\n",
    "30,35,40,K,E,22-12-21 17:57:43\n",
    "40,45,50,N,C,22-12-21 17:57:43\n",
    "100,105,110,M,C,22-12-21 17:57:43\n",
    "10,20,30,B,C,22-12-21 17:57:43\n",
    "20,30,35,A,D,22-12-21 17:57:43\n",
    "30,35,40,K,E,22-12-21 17:57:43\n",
    "40,45,50,N,C,22-12-21 17:57:43\n",
    "100,105,110,M,C,22-12-21 17:57:43\n",
    "10,20,30,B,C,22-12-21 17:57:43\n",
    "20,30,35,A,D,22-12-21 17:57:43\n",
    "30,35,40,K,E,22-12-21 17:57:43\n",
    "40,45,50,N,C,22-12-21 17:57:43\n",
    "100,105,110,M,C,22-12-21 17:57:43\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8240ede-5e3f-446d-98ef-5cd1b20b6b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strintdate(line):\n",
    "    col=[]\n",
    "    for x in line:\n",
    "        try:\n",
    "            try:\n",
    "                try:\n",
    "                    col.append(int(x))\n",
    "                except:\n",
    "                    col.append(datetime.datetime.strptime(x,\"%y-%m-%d %H:%M:%S\"))\n",
    "            except:\n",
    "                 col.append(x)\n",
    "        except:pass\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "053244a3-5956-4abf-a097-f4ffe325ad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(line):\n",
    "    col=[]\n",
    "    sums=0\n",
    "    for x in line:\n",
    "        col.append(x)\n",
    "    arr=[]\n",
    "    for i,x in enumerate(line):\n",
    "        if i<=2:\n",
    "            arr.append(int(x))\n",
    "        else:\n",
    "            pass\n",
    "    col.append(np.round(np.mean(arr)))    \n",
    "    col.append(np.round(np.std(arr)))\n",
    "    col.append(np.round(np.var(arr))) \n",
    "    col.append(np.round(np.min(arr)))    \n",
    "    col.append(np.round(np.max(arr)))    \n",
    "    col.append(np.round(np.ptp(arr)))      \n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34b23ec0-90f5-41bf-ad48-b30a62c35c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 20, 30, 'B', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 20.0, 8.0, 67.0, 10, 30, 20]\n",
      "[20, 30, 35, 'A', 'D', datetime.datetime(2022, 12, 21, 17, 57, 43), 28.0, 6.0, 39.0, 20, 35, 15]\n",
      "[30, 35, 40, 'K', 'E', datetime.datetime(2022, 12, 21, 17, 57, 43), 35.0, 4.0, 17.0, 30, 40, 10]\n",
      "[40, 45, 50, 'N', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 45.0, 4.0, 17.0, 40, 50, 10]\n",
      "[100, 105, 110, 'M', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 105.0, 4.0, 17.0, 100, 110, 10]\n",
      "[10, 20, 30, 'B', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 20.0, 8.0, 67.0, 10, 30, 20]\n",
      "[20, 30, 35, 'A', 'D', datetime.datetime(2022, 12, 21, 17, 57, 43), 28.0, 6.0, 39.0, 20, 35, 15]\n",
      "[30, 35, 40, 'K', 'E', datetime.datetime(2022, 12, 21, 17, 57, 43), 35.0, 4.0, 17.0, 30, 40, 10]\n",
      "[40, 45, 50, 'N', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 45.0, 4.0, 17.0, 40, 50, 10]\n",
      "[100, 105, 110, 'M', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 105.0, 4.0, 17.0, 100, 110, 10]\n",
      "[10, 20, 30, 'B', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 20.0, 8.0, 67.0, 10, 30, 20]\n",
      "[20, 30, 35, 'A', 'D', datetime.datetime(2022, 12, 21, 17, 57, 43), 28.0, 6.0, 39.0, 20, 35, 15]\n",
      "[30, 35, 40, 'K', 'E', datetime.datetime(2022, 12, 21, 17, 57, 43), 35.0, 4.0, 17.0, 30, 40, 10]\n",
      "[40, 45, 50, 'N', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 45.0, 4.0, 17.0, 40, 50, 10]\n",
      "[100, 105, 110, 'M', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 105.0, 4.0, 17.0, 100, 110, 10]\n",
      "[10, 20, 30, 'B', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 20.0, 8.0, 67.0, 10, 30, 20]\n",
      "[20, 30, 35, 'A', 'D', datetime.datetime(2022, 12, 21, 17, 57, 43), 28.0, 6.0, 39.0, 20, 35, 15]\n",
      "[30, 35, 40, 'K', 'E', datetime.datetime(2022, 12, 21, 17, 57, 43), 35.0, 4.0, 17.0, 30, 40, 10]\n",
      "[40, 45, 50, 'N', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 45.0, 4.0, 17.0, 40, 50, 10]\n",
      "[100, 105, 110, 'M', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 105.0, 4.0, 17.0, 100, 110, 10]\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    # | beam.Create([10, 20, 30, 40, 50])\n",
    "  (p | beam.io.ReadFromText('inputtext4.txt')\n",
    "     | beam.Map(lambda line : line.split(\",\"))\n",
    "     | beam.Map(lambda line : strintdate(line))\n",
    "     | beam.Map(lambda line : stats(line))\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b178b2d1-a78b-439a-9610-0a8bbf43475e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordsAlphabet(alphabet:'a', fruit='apple', country='australia', nationality='australian')\n",
      "WordsAlphabet(alphabet:'b', fruit='banana', country='brazil', nationality='brazilian')\n",
      "WordsAlphabet(alphabet:'c', fruit='cherry', country='canada', nationality='canadian')\n",
      "WordsAlphabet(alphabet:'d', fruit='doctor', country='duchland', nationality='duch')\n"
     ]
    }
   ],
   "source": [
    "#CoGroupByKey\n",
    "\n",
    "class WordsAlphabet:\n",
    "\n",
    "    def __init__(self, alphabet, fruit, country,nationality):\n",
    "        self.alphabet = alphabet\n",
    "        self.fruit = fruit\n",
    "        self.country = country\n",
    "        self.nationality = nationality\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"WordsAlphabet(alphabet:'%s', fruit='%s', country='%s', nationality='%s')\" % (self.alphabet, self.fruit, self.country,self.nationality)\n",
    "\n",
    "\n",
    "def apply_transforms(fruits, countries,nationalities):\n",
    "    def map_to_alphabet_kv(word):\n",
    "        return (word[0], word)\n",
    "\n",
    "    def cogbk_result_to_wordsalphabet(cgbk_result):\n",
    "        (alphabet, words) = cgbk_result\n",
    "        return WordsAlphabet(alphabet, words['fruits'][0], words['countries'][0],words['nationalities'][0])\n",
    "\n",
    "    fruits_kv = (fruits | 'Fruit to KV' >> beam.Map(map_to_alphabet_kv))\n",
    "    countries_kv = (countries | 'Country to KV' >> beam.Map(map_to_alphabet_kv))\n",
    "    nationalities_kv = (nationalities | 'nationality to KV' >> beam.Map(map_to_alphabet_kv))\n",
    "    return ({'fruits': fruits_kv, 'countries': countries_kv,'nationalities': nationalities_kv}\n",
    "            | beam.CoGroupByKey()\n",
    "            | beam.Map(cogbk_result_to_wordsalphabet))\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "    fruits = p | 'Fruits' >> beam.Create(['apple', 'banana', 'cherry','doctor'])\n",
    "    countries = p | 'Countries' >> beam.Create(['australia', 'brazil', 'canada','duchland'])\n",
    "    nationalities = p | 'Nationalities' >> beam.Create(['australian', 'brazilian', 'canadian','duch'])\n",
    "\n",
    "    (apply_transforms(fruits, countries,nationalities)\n",
    "    | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddda3281-6f17-4af7-97de-f80a349af064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "example_list=[10, 20, 50, 70, 90]\n",
    "\n",
    "\n",
    "class AverageFn(beam.CombineFn):\n",
    "\n",
    "    def create_accumulator(self):\n",
    "        return 0.0, 0\n",
    "\n",
    "    def add_input(self, accumulator, element):\n",
    "        (sum, i) = accumulator\n",
    "        # print(accumulator)\n",
    "        # print(element)\n",
    "        return sum + element, i + 1\n",
    "\n",
    "    def merge_accumulators(self, accumulators):\n",
    "        sums, counts = zip(*accumulators)\n",
    "        print(accumulators)\n",
    "        return sum(sums), sum(counts)\n",
    "\n",
    "    def extract_output(self, accumulator):\n",
    "        (sum, count) = accumulator\n",
    "        return sum / count if count else float('NaN')\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    clear_output()\n",
    "    (p | beam.Create(example_list)\n",
    "     | beam.CombineGlobally(AverageFn())\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*\n",
    "np.mean(example_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9489291-aa53-437b-96c9-d8cf10dd9a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.5\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as p:\n",
    "  (p | beam.Create(range(1, 11))\n",
    "     | beam.combiners.Mean.Globally()\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aadeb62d-2ae1-4b2e-8bf3-ced76462b771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.0\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as p:\n",
    "  (p | beam.Create(example_list)\n",
    "     | beam.combiners.Mean.Globally()\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3f9d31e-5614-4702-8a94-df863f644c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as p:\n",
    "  (p | beam.Create(range(1, 11))\n",
    "     | beam.combiners.Count.Globally()\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6a84751-776d-4a45-bd4c-3785508b90d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as p:\n",
    "  (p | beam.Create(range(1, 11))\n",
    "     | beam.combiners.Latest.Globally()\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb0e74c5-1575-459d-996c-cc08267c2822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as p:\n",
    "  (p | beam.Create(range(1, 11))\n",
    "     | beam.combiners.ToList()\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc6f0a79-190c-4170-bbba-ab6206370351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event(1, book-order_1, 2020-03-01 01:19:00+00:00)\n",
      "Event(2, book-order_2, 2020-03-02 02:18:00+00:00)\n",
      "Event(3, book-order_3, 2020-03-03 03:17:00+00:00)\n",
      "Event(4, book-order_4, 2020-03-04 04:16:00+00:00)\n",
      "Event(5, book-order_5, 2020-03-05 05:15:00+00:00)\n",
      "Event(6, book-order_6, 2020-03-06 06:14:00+00:00)\n",
      "Event(7, book-order_7, 2020-03-07 07:13:00+00:00)\n",
      "Event(8, book-order_8, 2020-03-08 08:12:00+00:00)\n",
      "Event(9, book-order_9, 2020-03-09 09:11:00+00:00)\n",
      "Event(10, book-order_10, 2020-03-10 10:10:00+00:00)\n",
      "Event(11, book-order_11, 2020-03-11 11:09:00+00:00)\n",
      "Event(12, book-order_12, 2020-03-12 12:08:00+00:00)\n",
      "Event(13, book-order_13, 2020-03-13 13:07:00+00:00)\n",
      "Event(14, book-order_14, 2020-03-14 14:06:00+00:00)\n",
      "Event(15, book-order_15, 2020-03-15 15:05:00+00:00)\n",
      "Event(16, book-order_16, 2020-03-16 16:04:00+00:00)\n",
      "Event(17, book-order_17, 2020-03-17 17:03:00+00:00)\n",
      "Event(18, book-order_18, 2020-03-18 18:02:00+00:00)\n"
     ]
    }
   ],
   "source": [
    "#   Licensed to the Apache Software Foundation (ASF) under one\n",
    "#   or more contributor license agreements.  See the NOTICE file\n",
    "#   distributed with this work for additional information\n",
    "#   regarding copyright ownership.  The ASF licenses this file\n",
    "#   to you under the Apache License, Version 2.0 (the\n",
    "#   \"License\"); you may not use this file except in compliance\n",
    "#   with the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#       http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#   Unless required by applicable law or agreed to in writing, software\n",
    "#   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#   See the License for the specific language governing permissions and\n",
    "#   limitations under the License.\n",
    "\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.transforms import window\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Event:\n",
    "    def __init__(self, id, event, timestamp):\n",
    "        self.id = id\n",
    "        self.event = event\n",
    "        self.timestamp = timestamp\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'Event({self.id}, {self.event}, {self.timestamp})'\n",
    "\n",
    "\n",
    "class AddTimestampDoFn(beam.DoFn):\n",
    "\n",
    "    def process(self, element, **kwargs):\n",
    "        unix_timestamp = element.timestamp.timestamp()\n",
    "        yield window.TimestampedValue(element, unix_timestamp)\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "  (p | beam.Create(\n",
    "      [Event(str(x), f'book-order_{x}', datetime.datetime(2020, 3, x, x, 20-x, 0, 0, tzinfo=pytz.UTC)) \\\n",
    "       for x in range(1,19) ] \n",
    "  )\n",
    "     | beam.ParDo(AddTimestampDoFn())\n",
    "    | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e1ed8fb-4670-42d3-9a15-77d16abed394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile  wordcount.py\n",
    "#\n",
    "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
    "# contributor license agreements.  See the NOTICE file distributed with\n",
    "# this work for additional information regarding copyright ownership.\n",
    "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
    "# (the \"License\"); you may not use this file except in compliance with\n",
    "# the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "\"\"\"A word-counting workflow.\"\"\"\n",
    "\n",
    "# pytype: skip-file\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "\n",
    "class WordExtractingDoFn(beam.DoFn):\n",
    "  \"\"\"Parse each line of input text into words.\"\"\"\n",
    "  def process(self, element):\n",
    "    \"\"\"Returns an iterator over the words of this element.\n",
    "\n",
    "    The element is a line of text.  If the line is blank, note that, too.\n",
    "\n",
    "    Args:\n",
    "      element: the element being processed\n",
    "\n",
    "    Returns:\n",
    "      The processed element.\n",
    "    \"\"\"\n",
    "    # return re.findall(r'[\\w\\']+', element, re.UNICODE)\n",
    "    return element.split()\n",
    "    \n",
    "\n",
    "\n",
    "def run(argv=None, save_main_session=True):\n",
    "  \"\"\"Main entry point; defines and runs the wordcount pipeline.\"\"\"\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--input',\n",
    "      dest='input',\n",
    "      default='gs://dataflow-samples/shakespeare/kinglear.txt',\n",
    "      \n",
    "      help='Input file to process.')\n",
    "  parser.add_argument(\n",
    "      '--output',\n",
    "      dest='output',\n",
    "      required=True,\n",
    "      help='Output file to write results to.')\n",
    "  known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "\n",
    "  # We use the save_main_session option because one or more DoFn's in this\n",
    "  # workflow rely on global context (e.g., a module imported at module level).\n",
    "  pipeline_options = PipelineOptions(pipeline_args)\n",
    "  pipeline_options.view_as(SetupOptions).save_main_session = save_main_session\n",
    "\n",
    "  # The pipeline will be run on exiting the with block.\n",
    "  with beam.Pipeline(options=pipeline_options) as p:\n",
    "\n",
    "    # Read the text file[pattern] into a PCollection.\n",
    "    lines = p | 'Read' >> ReadFromText(known_args.input)\n",
    "\n",
    "    counts = (\n",
    "        lines\n",
    "        | 'Split' >> (beam.ParDo(WordExtractingDoFn()).with_output_types(str))\n",
    "        | 'PairWithOne' >> beam.Map(lambda x: (x, 1))\n",
    "        | 'GroupAndSum' >> beam.CombinePerKey(sum))\n",
    "\n",
    "    # Format the counts into a PCollection of strings.\n",
    "    def format_result(word, count):\n",
    "      return '%s: %d' % (word, count)\n",
    "\n",
    "    output = counts | 'Format' >> beam.MapTuple(format_result)\n",
    "\n",
    "    # Write the output using a \"Write\" transform that has side effects.\n",
    "    # pylint: disable=expression-not-assigned\n",
    "    output | 'Write' >> WriteToText(known_args.output)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  logging.getLogger().setLevel(logging.INFO)\n",
    "  run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5da91863-ab00-4e9b-a904-6d646eba7fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> wordcount_output-00000-of-00001 <==\n",
      "KING: 242\n",
      "LEAR: 222\n",
      "DRAMATIS: 1\n",
      "PERSONAE: 1\n",
      "king: 29\n",
      "\n",
      "==> wordcount_output2-00000-of-00001 <==\n",
      "KING: 243\n",
      "LEAR: 236\n",
      "DRAMATIS: 1\n",
      "PERSONAE: 1\n",
      "king: 65\n"
     ]
    }
   ],
   "source": [
    "!python3 wordcount.py --output wordcount_output\n",
    "clear_output()\n",
    "!head -n 5 wordcount_output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43bcba77-32df-43b4-a13f-d5b9de946748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount_with_metric.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount_with_metric.py\n",
    "#\n",
    "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
    "# contributor license agreements.  See the NOTICE file distributed with\n",
    "# this work for additional information regarding copyright ownership.\n",
    "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
    "# (the \"License\"); you may not use this file except in compliance with\n",
    "# the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "\"\"\"A word-counting workflow.\"\"\"\n",
    "\n",
    "# pytype: skip-file\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.metrics import Metrics\n",
    "from apache_beam.metrics.metric import MetricsFilter\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "\n",
    "class WordExtractingDoFn(beam.DoFn):\n",
    "  \"\"\"Parse each line of input text into words.\"\"\"\n",
    "  def __init__(self):\n",
    "    # TODO(BEAM-6158): Revert the workaround once we can pickle super() on py3.\n",
    "    # super().__init__()\n",
    "    beam.DoFn.__init__(self)\n",
    "    self.words_counter = Metrics.counter(self.__class__, 'words')\n",
    "    self.word_lengths_counter = Metrics.counter(self.__class__, 'word_lengths')\n",
    "    self.word_lengths_dist = Metrics.distribution(\n",
    "        self.__class__, 'word_len_dist')\n",
    "    self.empty_line_counter = Metrics.counter(self.__class__, 'empty_lines')\n",
    "\n",
    "  def process(self, element):\n",
    "    \"\"\"Returns an iterator over the words of this element.\n",
    "\n",
    "    The element is a line of text.  If the line is blank, note that, too.\n",
    "\n",
    "    Args:\n",
    "      element: the element being processed\n",
    "\n",
    "    Returns:\n",
    "      The processed element.\n",
    "    \"\"\"\n",
    "    text_line = element.strip()\n",
    "    if not text_line:\n",
    "      self.empty_line_counter.inc(1)\n",
    "    words = re.findall(r'[\\w\\']+', text_line, re.UNICODE)\n",
    "    for w in words:\n",
    "      self.words_counter.inc()\n",
    "      self.word_lengths_counter.inc(len(w))\n",
    "      self.word_lengths_dist.update(len(w))\n",
    "    return words\n",
    "\n",
    "\n",
    "def main(argv=None, save_main_session=True):\n",
    "  \"\"\"Main entry point; defines and runs the wordcount pipeline.\"\"\"\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--input',\n",
    "      dest='input',\n",
    "      default='gs://dataflow-samples/shakespeare/kinglear.txt',\n",
    "      help='Input file to process.')\n",
    "  parser.add_argument(\n",
    "      '--output',\n",
    "      dest='output',\n",
    "      required=True,\n",
    "      help='Output file to write results to.')\n",
    "  known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "\n",
    "  # We use the save_main_session option because one or more DoFn's in this\n",
    "  # workflow rely on global context (e.g., a module imported at module level).\n",
    "  pipeline_options = PipelineOptions(pipeline_args)\n",
    "  pipeline_options.view_as(SetupOptions).save_main_session = save_main_session\n",
    "  p = beam.Pipeline(options=pipeline_options)\n",
    "\n",
    "  # Read the text file[pattern] into a PCollection.\n",
    "  lines = p | 'read' >> ReadFromText(known_args.input)\n",
    "\n",
    "  # Count the occurrences of each word.\n",
    "  def count_ones(word_ones):\n",
    "    (word, ones) = word_ones\n",
    "    return (word, sum(ones))\n",
    "\n",
    "  counts = (\n",
    "      lines\n",
    "      | 'split' >> (beam.ParDo(WordExtractingDoFn()).with_output_types(str))\n",
    "      | 'pair_with_one' >> beam.Map(lambda x: (x, 1))\n",
    "      | 'group' >> beam.GroupByKey()\n",
    "      | 'count' >> beam.Map(count_ones))\n",
    "\n",
    "  # Format the counts into a PCollection of strings.\n",
    "  def format_result(word_count):\n",
    "    (word, count) = word_count\n",
    "    return '%s: %d' % (word, count)\n",
    "\n",
    "  output = counts | 'format' >> beam.Map(format_result)\n",
    "\n",
    "  # Write the output using a \"Write\" transform that has side effects.\n",
    "  # pylint: disable=expression-not-assigned\n",
    "  output | 'write' >> WriteToText(known_args.output)\n",
    "\n",
    "  result = p.run()\n",
    "  result.wait_until_finish()\n",
    "\n",
    "  # Do not query metrics when creating a template which doesn't run\n",
    "  if (not hasattr(result, 'has_job')  # direct runner\n",
    "      or result.has_job):  # not just a template creation\n",
    "    empty_lines_filter = MetricsFilter().with_name('empty_lines')\n",
    "    query_result = result.metrics().query(empty_lines_filter)\n",
    "    if query_result['counters']:\n",
    "      empty_lines_counter = query_result['counters'][0]\n",
    "      logging.info('number of empty lines: %d', empty_lines_counter.result)\n",
    "\n",
    "    word_lengths_filter = MetricsFilter().with_name('word_len_dist')\n",
    "    query_result = result.metrics().query(word_lengths_filter)\n",
    "    if query_result['distributions']:\n",
    "      word_lengths_dist = query_result['distributions'][0]\n",
    "      logging.info('average word length: %d', word_lengths_dist.result.mean)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  logging.getLogger().setLevel(logging.INFO)\n",
    "  main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4451586e-5ccd-4358-85ad-dc51a0dc15df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KING: 243\n",
      "LEAR: 236\n",
      "DRAMATIS: 1\n",
      "PERSONAE: 1\n",
      "king: 65\n"
     ]
    }
   ],
   "source": [
    "!python3 wordcount_with_metric.py --output wordcount_output2\n",
    "clear_output()\n",
    "!head -n 5 wordcount_output2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92f8dea4-81d8-4d8b-ab15-b26a1b2563b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcounttest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcounttest.py\n",
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
    "# contributor license agreements.  See the NOTICE file distributed with\n",
    "# this work for additional information regarding copyright ownership.\n",
    "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
    "# (the \"License\"); you may not use this file except in compliance with\n",
    "# the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "\"\"\"Test for the wordcount example.\"\"\"\n",
    "\n",
    "# pytype: skip-file\n",
    "\n",
    "import collections\n",
    "import logging\n",
    "import re\n",
    "import tempfile\n",
    "import unittest\n",
    "\n",
    "import pytest\n",
    "\n",
    "from apache_beam.examples import wordcount\n",
    "from apache_beam.testing.util import open_shards\n",
    "\n",
    "\n",
    "@pytest.mark.examples_postcommit\n",
    "class WordCountTest(unittest.TestCase):\n",
    "\n",
    "  SAMPLE_TEXT = (\n",
    "      u'a b c a b a\\nacento gr√°fico\\nJuly 30, 2018\\n\\n aa bb cc aa bb aa')\n",
    "\n",
    "  def create_temp_file(self, contents):\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "      f.write(contents.encode('utf-8'))\n",
    "      return f.name\n",
    "\n",
    "  def test_basics(self):\n",
    "    temp_path = self.create_temp_file(self.SAMPLE_TEXT)\n",
    "    expected_words = collections.defaultdict(int)\n",
    "    for word in re.findall(r'[\\w\\']+', self.SAMPLE_TEXT, re.UNICODE):\n",
    "      expected_words[word] += 1\n",
    "    wordcount.run(['--input=%s*' % temp_path, '--output=%s.result' % temp_path],\n",
    "                  save_main_session=False)\n",
    "    # Parse result file and compare.\n",
    "    results = []\n",
    "    with open_shards(temp_path + '.result-*-of-*') as result_file:\n",
    "      for line in result_file:\n",
    "        match = re.search(r'(\\S+): ([0-9]+)', line)\n",
    "        if match is not None:\n",
    "          results.append((match.group(1), int(match.group(2))))\n",
    "    self.assertEqual(sorted(results), sorted(expected_words.items()))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  logging.getLogger().setLevel(logging.INFO)\n",
    "  unittest.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05a5b89f-9a6e-47fa-a562-dcca2451e03e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.43.0\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f761beeaaf0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f761beeac10> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f761beeb160> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f761beeb1f0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f761beeb3a0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f761beeb430> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f761beeb550> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f761beeb5e0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f761beeb670> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f761beeb700> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f761beeb940> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function add_impulse_to_dangling_transforms at 0x7f761beeba60> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f761beeb8b0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f761beeb9d0> ====================\n",
      "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f761be898b0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.00 seconds.\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.553s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 wordcounttest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95368eba-ef67-4b70-b57c-bab0e0562f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', ['apple', 'ant'])\n",
      "('b', ['ball', 'bear'])\n",
      "('c', ['car', 'cheetah'])\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "  (p | beam.Create(['apple', 'ball', 'car', 'bear', 'cheetah', 'ant'])\n",
    "     | beam.Map(lambda word: (word[0], word))\n",
    "     | beam.GroupByKey()\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e7fc12b-a715-410f-9e23-e966a2aec1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordsAlphabet(alphabet:'a', fruit='apple', country='australia')\n",
      "WordsAlphabet(alphabet:'b', fruit='banana', country='brazil')\n",
      "WordsAlphabet(alphabet:'c', fruit='cherry', country='canada')\n"
     ]
    }
   ],
   "source": [
    "#   Licensed to the Apache Software Foundation (ASF) under one\n",
    "#   or more contributor license agreements.  See the NOTICE file\n",
    "#   distributed with this work for additional information\n",
    "#   regarding copyright ownership.  The ASF licenses this file\n",
    "#   to you under the Apache License, Version 2.0 (the\n",
    "#   \"License\"); you may not use this file except in compliance\n",
    "#   with the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#       http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#   Unless required by applicable law or agreed to in writing, software\n",
    "#   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#   See the License for the specific language governing permissions and\n",
    "#   limitations under the License.\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "class WordsAlphabet:\n",
    "\n",
    "    def __init__(self, alphabet, fruit, country):\n",
    "        self.alphabet = alphabet\n",
    "        self.fruit = fruit\n",
    "        self.country = country\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"WordsAlphabet(alphabet:'%s', fruit='%s', country='%s')\" % (self.alphabet, self.fruit, self.country)\n",
    "\n",
    "\n",
    "def apply_transforms(fruits, countries):\n",
    "    def map_to_alphabet_kv(word):\n",
    "        return (word[0], word)\n",
    "\n",
    "    def cogbk_result_to_wordsalphabet(cgbk_result):\n",
    "        (alphabet, words) = cgbk_result\n",
    "        return WordsAlphabet(alphabet, words['fruits'][0], words['countries'][0])\n",
    "\n",
    "    fruits_kv = (fruits | 'Fruit to KV' >> beam.Map(map_to_alphabet_kv))\n",
    "    countries_kv = (countries | 'Country to KV' >> beam.Map(map_to_alphabet_kv))\n",
    "\n",
    "    return ({'fruits': fruits_kv, 'countries': countries_kv}\n",
    "            | beam.CoGroupByKey()\n",
    "            | beam.Map(cogbk_result_to_wordsalphabet))\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "  fruits = p | 'Fruits' >> beam.Create(['apple', 'banana', 'cherry'])\n",
    "  countries = p | 'Countries' >> beam.Create(['australia', 'brazil', 'canada'])\n",
    "\n",
    "  (apply_transforms(fruits, countries)\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b115eb3-ee0a-4373-ad64-6179c81a8ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Player 1', 115)\n",
      "('Player 2', 85)\n",
      "('Player 3', 25)\n"
     ]
    }
   ],
   "source": [
    "#combine per key\n",
    "import apache_beam as beam\n",
    "\n",
    "PLAYER_1 = 'Player 1'\n",
    "PLAYER_2 = 'Player 2'\n",
    "PLAYER_3 = 'Player 3'\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "  (p | beam.Create([(PLAYER_1, 15), (PLAYER_2, 10), (PLAYER_1, 100),\n",
    "                    (PLAYER_3, 25), (PLAYER_2, 75)])\n",
    "     | beam.CombinePerKey(sum)\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e040642-f2e7-448c-bd4d-801cab2b4864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "#combine simple function\n",
    "import apache_beam as beam\n",
    "\n",
    "def sum(numbers):\n",
    "    total = 0\n",
    "\n",
    "    for num in numbers:\n",
    "        total += num\n",
    "\n",
    "    return total\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "  (p | beam.Create([1, 2, 3, 4, 5])\n",
    "     | beam.CombineGlobally(sum)\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6c49e22a-a224-49fb-b1a8-5ed7fbc988b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person[Henry,Singapore,Singapore,Asia,25,50,37.5]\n",
      "Person[Jane,San Francisco,United States,America,30,45,37.5]\n",
      "Person[Lee,Beijing,China,Asia,35,35,35.0]\n",
      "Person[John,Sydney,Australia,Australia,45,30,37.5]\n",
      "Person[Alfred,London,United Kingdom,Europe,50,25,37.5]\n"
     ]
    }
   ],
   "source": [
    "#side input\n",
    "import apache_beam as beam\n",
    "\n",
    "class Person:\n",
    "    def __init__(self, name, city, country='',continent='',num1=\"\",num2=\"\",avg=\"\"):\n",
    "        self.name = name\n",
    "        self.city = city\n",
    "        self.country = country\n",
    "        self.continent= continent\n",
    "        self.num1=num1\n",
    "        self.num2=num2\n",
    "        self.avg=avg      \n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Person[' + self.name + ',' + self.city + ',' + self.country + ',' + self.continent +',' + self.num1+',' + self.num2+','  +self.avg+']'\n",
    "\n",
    "\n",
    "class EnrichCountryDoFn(beam.DoFn):\n",
    "\n",
    "    def process(self, element, cities_to_countries):\n",
    "        yield Person(element.name, element.city,\n",
    "                     cities_to_countries[element.city])\n",
    "        \n",
    "class EnrichContinentDoFn(beam.DoFn):\n",
    "\n",
    "    def process(self, element, cities_to_continents):\n",
    "        yield Person(element.name, element.city,element.country,\n",
    "                     cities_to_continents[element.city])\n",
    "        \n",
    "class NumDoFn(beam.DoFn):\n",
    "\n",
    "    def process(self, element, name_to_num1):\n",
    "        yield Person(element.name, element.city,element.country,element.continent,\n",
    "                     name_to_num1[element.name])\n",
    "        \n",
    "class NumDoFn2(beam.DoFn):\n",
    "\n",
    "    def process(self, element, name_to_num1):\n",
    "        yield Person(element.name, element.city,element.country,element.continent,element.num1,\n",
    "                     name_to_num2[element.name])\n",
    "\n",
    "class AvgDoFn(beam.DoFn):\n",
    "\n",
    "    def process(self, element):\n",
    "        yield Person(element.name, element.city,element.country,element.continent,element.num1,element.num2,\n",
    "                     str(round((int(element.num1)+int(element.num2))/2,2)))\n",
    "        \n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "    cities_to_countries = {\n",
    "      'Beijing': 'China',\n",
    "      'London': 'United Kingdom',\n",
    "      'San Francisco': 'United States',\n",
    "      'Singapore': 'Singapore',\n",
    "      'Sydney': 'Australia'\n",
    "    }\n",
    "\n",
    "    cities_to_continents = {\n",
    "      'Beijing': 'Asia',\n",
    "      'London': 'Europe',\n",
    "      'San Francisco': 'America',\n",
    "      'Singapore': 'Asia',\n",
    "      'Sydney': 'Australia'\n",
    "    }\n",
    "    \n",
    "    name_to_num1 = {\n",
    "      'Henry': \"25\",\n",
    "      'Jane': \"30\",\n",
    "      'Lee': \"35\",\n",
    "      'John': \"45\",\n",
    "      'Alfred': \"50\"\n",
    "    }\n",
    "    \n",
    "    name_to_num2 = {\n",
    "      'Henry': \"50\",\n",
    "      'Jane': \"45\",\n",
    "      'Lee': \"35\",\n",
    "      'John': \"30\",\n",
    "      'Alfred': \"25\"\n",
    "    }\n",
    "    \n",
    "    persons = [\n",
    "      Person('Henry', 'Singapore'),\n",
    "      Person('Jane', 'San Francisco'),\n",
    "      Person('Lee', 'Beijing'),\n",
    "      Person('John', 'Sydney'),\n",
    "      Person('Alfred', 'London')\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    (p  | 'Create' >> beam.Create(persons)\n",
    "        | 'Add-countries' >> beam.ParDo(EnrichCountryDoFn(), cities_to_countries)\n",
    "        | 'Add-continents' >> beam.ParDo(EnrichContinentDoFn(), cities_to_continents)\n",
    "        | 'Add-num' >> beam.ParDo(NumDoFn(), name_to_num1)\n",
    "        | 'Add-num2' >> beam.ParDo(NumDoFn2(), name_to_num2)\n",
    "        | 'avg' >> beam.ParDo(AvgDoFn())     \n",
    "        | 'Write' >> beam.io.textio.WriteToText('example-output'))\n",
    "    \n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b45d3b5-617f-473c-b2eb-4ed0e8f98d64",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c477fa82-fad0-4280-98ad-325678957e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing filtertest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile filtertest.py\n",
    "#filter test\n",
    "import logging\n",
    "import time\n",
    "import unittest\n",
    "\n",
    "import pytest\n",
    "from hamcrest.core.core.allof import all_of\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.examples.cookbook import filters\n",
    "from apache_beam.io.gcp.tests import utils\n",
    "from apache_beam.io.gcp.tests.bigquery_matcher import BigqueryMatcher\n",
    "from apache_beam.testing.pipeline_verifiers import PipelineStateMatcher\n",
    "from apache_beam.testing.test_pipeline import TestPipeline\n",
    "from apache_beam.testing.util import assert_that\n",
    "from apache_beam.testing.util import equal_to\n",
    "\n",
    "\n",
    "class FiltersTest(unittest.TestCase):\n",
    "\n",
    "  # The default checksum is a SHA-1 hash generated from sorted rows reading\n",
    "  # from expected Bigquery table.\n",
    "  DEFAULT_CHECKSUM = '813b6da1624334732fad4467d74a7c8a62559c6b'\n",
    "\n",
    "  # Note that 'removed' should be projected away by the pipeline\n",
    "  input_data = [\n",
    "      {\n",
    "          'year': 2010, 'month': 1, 'day': 1, 'mean_temp': 3, 'removed': 'a'\n",
    "      },\n",
    "      {\n",
    "          'year': 2012, 'month': 1, 'day': 2, 'mean_temp': 3, 'removed': 'a'\n",
    "      },\n",
    "      {\n",
    "          'year': 2011, 'month': 1, 'day': 3, 'mean_temp': 5, 'removed': 'a'\n",
    "      },\n",
    "      {\n",
    "          'year': 2013, 'month': 2, 'day': 1, 'mean_temp': 3, 'removed': 'a'\n",
    "      },\n",
    "      {\n",
    "          'year': 2011, 'month': 3, 'day': 3, 'mean_temp': 5, 'removed': 'a'\n",
    "      },\n",
    "  ]\n",
    "\n",
    "  def _get_result_for_month(self, pipeline, month):\n",
    "    rows = (pipeline | 'create' >> beam.Create(self.input_data))\n",
    "    results = filters.filter_cold_days(rows, month)\n",
    "    return results\n",
    "\n",
    "  def test_basics(self):\n",
    "    \"\"\"Test that the correct result is returned for a simple dataset.\"\"\"\n",
    "    with TestPipeline() as p:\n",
    "      results = self._get_result_for_month(p, 1)\n",
    "      assert_that(\n",
    "          results,\n",
    "          equal_to([{\n",
    "              'year': 2010, 'month': 1, 'day': 1, 'mean_temp': 3\n",
    "          }, {\n",
    "              'year': 2012, 'month': 1, 'day': 2, 'mean_temp': 3\n",
    "          }]))\n",
    "\n",
    "  def test_basic_empty(self):\n",
    "    \"\"\"Test that the correct empty result is returned for a simple dataset.\"\"\"\n",
    "    with TestPipeline() as p:\n",
    "      results = self._get_result_for_month(p, 3)\n",
    "      assert_that(results, equal_to([]))\n",
    "\n",
    "  def test_basic_empty_missing(self):\n",
    "    \"\"\"Test that the correct empty result is returned for a missing month.\"\"\"\n",
    "    with TestPipeline() as p:\n",
    "      results = self._get_result_for_month(p, 4)\n",
    "      assert_that(results, equal_to([]))\n",
    "\n",
    "  @pytest.mark.examples_postcommit\n",
    "  def test_filters_output_bigquery_matcher(self):\n",
    "    test_pipeline = TestPipeline(is_integration_test=True)\n",
    "\n",
    "    # Set extra options to the pipeline for test purpose\n",
    "    project = test_pipeline.get_option('project')\n",
    "\n",
    "    dataset = 'FiltersTestIT'\n",
    "    table = 'cold_days_%s' % int(round(time.time() * 1000))\n",
    "    output_table = '.'.join([dataset, table])\n",
    "    query = 'SELECT year, month, day, mean_temp FROM `%s`' % output_table\n",
    "\n",
    "    pipeline_verifiers = [\n",
    "        PipelineStateMatcher(),\n",
    "        BigqueryMatcher(\n",
    "            project=project, query=query, checksum=self.DEFAULT_CHECKSUM)\n",
    "    ]\n",
    "    extra_opts = {\n",
    "        'output': output_table,\n",
    "        'on_success_matcher': all_of(*pipeline_verifiers)\n",
    "    }\n",
    "\n",
    "    # Register cleanup before pipeline execution.\n",
    "    # Note that actual execution happens in reverse order.\n",
    "    self.addCleanup(utils.delete_bq_table, project, dataset, table)\n",
    "\n",
    "    # Get pipeline options from command argument: --test-pipeline-options,\n",
    "    # and start pipeline job by calling pipeline main function.\n",
    "    filters.run(test_pipeline.get_full_options_as_args(**extra_opts))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  logging.getLogger().setLevel(logging.INFO)\n",
    "  unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1d0e0102-1627-4f61-8edd-c1f150780854",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.43.0\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f3e961dfc10> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f3e961dfd30> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f3e961e1280> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f3e961e1310> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f3e961e14c0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f3e961e1550> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f3e961e1670> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f3e961e1700> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f3e961e1790> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f3e961e1820> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f3e961e1a60> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function add_impulse_to_dangling_transforms at 0x7f3e9637d670> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f3e961e19d0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f3e961e1af0> ====================\n",
      "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f3e960b7d30> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      ".INFO:root:Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.43.0\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f3e961dfc10> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f3e961dfd30> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f3e961e1280> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f3e961e1310> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f3e961e14c0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f3e961e1550> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f3e961e1670> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f3e961e1700> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f3e961e1790> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f3e961e1820> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f3e961e1a60> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function add_impulse_to_dangling_transforms at 0x7f3e9637d670> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f3e961e19d0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f3e961e1af0> ====================\n",
      "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f3e94f4b6d0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      ".INFO:root:Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.43.0\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f3e961dfc10> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f3e961dfd30> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f3e961e1280> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f3e961e1310> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f3e961e14c0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f3e961e1550> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f3e961e1670> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f3e961e1700> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f3e961e1790> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f3e961e1820> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f3e961e1a60> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function add_impulse_to_dangling_transforms at 0x7f3e9637d670> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f3e961e19d0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f3e961e1af0> ====================\n",
      "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f3e946469a0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      ".s\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 2.494s\n",
      "\n",
      "OK (skipped=1)\n"
     ]
    }
   ],
   "source": [
    "!python3 filtertest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89bd125-df2a-481c-9538-833fda25b684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
