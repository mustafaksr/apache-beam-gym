{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19f571ff-1bc7-4912-bd38-9410885ad7a2",
   "metadata": {},
   "source": [
    "https://play.beam.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ace0d5-fa31-4af1-b011-4f60f3ecadf0",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed1476f3-7923-4c42-9714-8ffe367dc6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3acaca7-92ff-48ae-bebe-77ca9c5d9d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f34bd205-f61e-429e-9273-be026257bebd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as p:\n",
    "\n",
    "  (p | beam.Create(range(0, 11,2)[1:])\n",
    "     | beam.combiners.Mean.Globally()\n",
    "     | beam.io.textio.WriteToText('example-output')\n",
    "    )\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e7d9438-7fdc-4690-a8f9-f34a71b17d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ceefaf7-264e-4d27-b96c-29d1812dbd4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 2, 4, 6, 8, 10], 5.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0, 11,2)),np.mean(list(range(0, 11,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f73e6142-c70a-4c1d-9c9e-3acce2fa101c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 9, 8, 7]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "  (p | beam.Create(range(1, 11))\n",
    "     | beam.combiners.Top.Largest(4)\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "925b4add-8dab-4fd8-9f2f-dac2ed8b8462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "  (p | beam.Create(range(1, 11))\n",
    "     | beam.combiners.Top.Smallest(2)\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cd620f7-4508-4228-a85f-968e53448929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "  (p | beam.Create(range(1, 11))\n",
    "     | beam.CombineGlobally(min)\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc0effb3-380d-4039-8e85-e2daf51e4278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputtext.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile inputtext.txt\n",
    "10\n",
    "20\n",
    "30\n",
    "40\n",
    "100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d72280b3-e78e-49f0-8214-3c6d8c9b559e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "45\n",
      "65\n",
      "85\n",
      "205\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    # | beam.Create([10, 20, 30, 40, 50])\n",
    "  (p | beam.io.ReadFromText('inputtext.txt')\n",
    "     | beam.Map(lambda num: 2*int(num) + 5)\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "593ded3e-453e-456a-83dc-5efec4b330e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputtext2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile inputtext2.txt\n",
    "10,20,30\n",
    "20,30,35\n",
    "30,35,40\n",
    "40,45,50\n",
    "100,105,110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa61d5cf-97f5-4eb5-b918-b4f86720b0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 45, 65]\n",
      "[45, 65, 75]\n",
      "[65, 75, 85]\n",
      "[85, 95, 105]\n",
      "[205, 215, 225]\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    # | beam.Create([10, 20, 30, 40, 50])\n",
    "  (p | beam.io.ReadFromText('inputtext2.txt')\n",
    "     | beam.Map(lambda line: line.split(\",\"))\n",
    "     | beam.Map(lambda nums: [2*int(num) + 5 for num in nums])\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae38cc2c-6329-44c2-9539-3a31c31cfd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputtext3.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile inputtext3.txt\n",
    "10,20,30,B,C\n",
    "20,30,35,A,D\n",
    "30,35,40,K,E\n",
    "40,45,50,N,C\n",
    "100,105,110,M,C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f8a5e9c-6e07-46b0-a848-31192501c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strint(line):\n",
    "    col=[]\n",
    "    for x in line:\n",
    "        try:\n",
    "            col.append(int(x))\n",
    "        except:\n",
    "            col.append(x)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a22eac6-5f73-43e9-8ec3-12f2cf56e50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 20, 30, 'B', 'C']\n",
      "[20, 30, 35, 'A', 'D']\n",
      "[30, 35, 40, 'K', 'E']\n",
      "[40, 45, 50, 'N', 'C']\n",
      "[100, 105, 110, 'M', 'C']\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    # | beam.Create([10, 20, 30, 40, 50])\n",
    "  (p | beam.io.ReadFromText('inputtext3.txt')\n",
    "     | beam.Map(lambda line : line.split(\",\"))\n",
    "     | beam.Map(lambda num : strint(num))\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e78eff5-eba4-4268-9ea9-97371dd57212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e9f2a9b-a9c4-4395-b85b-800bc15715f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3818d21b-427a-473f-a974-8d88983d3caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 12, 21, 20, 27, 56, 628459)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.fromtimestamp(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62f3d9f0-67b6-425c-9582-a0ce9cc395b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 12, 21, 17, 57, 43)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.strptime(\"22-12-21 17:57:43\",\"%y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "340336d1-f904-4583-884e-1812e1a24006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputtext4.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile inputtext4.txt\n",
    "10,20,30,B,C,22-12-21 17:57:43\n",
    "20,30,35,A,D,22-12-21 17:57:43\n",
    "30,35,40,K,E,22-12-21 17:57:43\n",
    "40,45,50,N,C,22-12-21 17:57:43\n",
    "100,105,110,M,C,22-12-21 17:57:43\n",
    "10,20,30,B,C,22-12-21 17:57:43\n",
    "20,30,35,A,D,22-12-21 17:57:43\n",
    "30,35,40,K,E,22-12-21 17:57:43\n",
    "40,45,50,N,C,22-12-21 17:57:43\n",
    "100,105,110,M,C,22-12-21 17:57:43\n",
    "10,20,30,B,C,22-12-21 17:57:43\n",
    "20,30,35,A,D,22-12-21 17:57:43\n",
    "30,35,40,K,E,22-12-21 17:57:43\n",
    "40,45,50,N,C,22-12-21 17:57:43\n",
    "100,105,110,M,C,22-12-21 17:57:43\n",
    "10,20,30,B,C,22-12-21 17:57:43\n",
    "20,30,35,A,D,22-12-21 17:57:43\n",
    "30,35,40,K,E,22-12-21 17:57:43\n",
    "40,45,50,N,C,22-12-21 17:57:43\n",
    "100,105,110,M,C,22-12-21 17:57:43\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8240ede-5e3f-446d-98ef-5cd1b20b6b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strintdate(line):\n",
    "    col=[]\n",
    "    for x in line:\n",
    "        try:\n",
    "            try:\n",
    "                col.append(int(x))\n",
    "            except:\n",
    "                col.append(datetime.datetime.strptime(x,\"%y-%m-%d %H:%M:%S\"))\n",
    "        except:\n",
    "             col.append(x)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "053244a3-5956-4abf-a097-f4ffe325ad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(line):\n",
    "    col=[]\n",
    "    sums=0\n",
    "    for x in line:\n",
    "        col.append(x)\n",
    "    arr=[]\n",
    "    for i,x in enumerate(line):\n",
    "        if i<=2:\n",
    "            arr.append(int(x))\n",
    "        else:\n",
    "            pass\n",
    "    col.append(np.round(np.mean(arr)))    \n",
    "    col.append(np.round(np.std(arr)))\n",
    "    col.append(np.round(np.var(arr))) \n",
    "    col.append(np.round(np.min(arr)))    \n",
    "    col.append(np.round(np.max(arr)))    \n",
    "    col.append(np.round(np.ptp(arr)))      \n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34b23ec0-90f5-41bf-ad48-b30a62c35c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 20, 30, 'B', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 20.0, 8.0, 67.0, 10, 30, 20]\n",
      "[20, 30, 35, 'A', 'D', datetime.datetime(2022, 12, 21, 17, 57, 43), 28.0, 6.0, 39.0, 20, 35, 15]\n",
      "[30, 35, 40, 'K', 'E', datetime.datetime(2022, 12, 21, 17, 57, 43), 35.0, 4.0, 17.0, 30, 40, 10]\n",
      "[40, 45, 50, 'N', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 45.0, 4.0, 17.0, 40, 50, 10]\n",
      "[100, 105, 110, 'M', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 105.0, 4.0, 17.0, 100, 110, 10]\n",
      "[10, 20, 30, 'B', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 20.0, 8.0, 67.0, 10, 30, 20]\n",
      "[20, 30, 35, 'A', 'D', datetime.datetime(2022, 12, 21, 17, 57, 43), 28.0, 6.0, 39.0, 20, 35, 15]\n",
      "[30, 35, 40, 'K', 'E', datetime.datetime(2022, 12, 21, 17, 57, 43), 35.0, 4.0, 17.0, 30, 40, 10]\n",
      "[40, 45, 50, 'N', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 45.0, 4.0, 17.0, 40, 50, 10]\n",
      "[100, 105, 110, 'M', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 105.0, 4.0, 17.0, 100, 110, 10]\n",
      "[10, 20, 30, 'B', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 20.0, 8.0, 67.0, 10, 30, 20]\n",
      "[20, 30, 35, 'A', 'D', datetime.datetime(2022, 12, 21, 17, 57, 43), 28.0, 6.0, 39.0, 20, 35, 15]\n",
      "[30, 35, 40, 'K', 'E', datetime.datetime(2022, 12, 21, 17, 57, 43), 35.0, 4.0, 17.0, 30, 40, 10]\n",
      "[40, 45, 50, 'N', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 45.0, 4.0, 17.0, 40, 50, 10]\n",
      "[100, 105, 110, 'M', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 105.0, 4.0, 17.0, 100, 110, 10]\n",
      "[10, 20, 30, 'B', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 20.0, 8.0, 67.0, 10, 30, 20]\n",
      "[20, 30, 35, 'A', 'D', datetime.datetime(2022, 12, 21, 17, 57, 43), 28.0, 6.0, 39.0, 20, 35, 15]\n",
      "[30, 35, 40, 'K', 'E', datetime.datetime(2022, 12, 21, 17, 57, 43), 35.0, 4.0, 17.0, 30, 40, 10]\n",
      "[40, 45, 50, 'N', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 45.0, 4.0, 17.0, 40, 50, 10]\n",
      "[100, 105, 110, 'M', 'C', datetime.datetime(2022, 12, 21, 17, 57, 43), 105.0, 4.0, 17.0, 100, 110, 10]\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    # | beam.Create([10, 20, 30, 40, 50])\n",
    "  (p | beam.io.ReadFromText('inputtext4.txt')\n",
    "     | beam.Map(lambda line : line.split(\",\"))\n",
    "     | beam.Map(lambda line : strintdate(line))\n",
    "     | beam.Map(lambda line : stats(line))\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b178b2d1-a78b-439a-9610-0a8bbf43475e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordsAlphabet(alphabet:'a', fruit='apple', country='australia', nationality='australian')\n",
      "WordsAlphabet(alphabet:'b', fruit='banana', country='brazil', nationality='brazilian')\n",
      "WordsAlphabet(alphabet:'c', fruit='cherry', country='canada', nationality='canadian')\n",
      "WordsAlphabet(alphabet:'d', fruit='doctor', country='duchland', nationality='duch')\n"
     ]
    }
   ],
   "source": [
    "#CoGroupByKey\n",
    "\n",
    "class WordsAlphabet:\n",
    "\n",
    "    def __init__(self, alphabet, fruit, country,nationality):\n",
    "        self.alphabet = alphabet\n",
    "        self.fruit = fruit\n",
    "        self.country = country\n",
    "        self.nationality = nationality\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"WordsAlphabet(alphabet:'%s', fruit='%s', country='%s', nationality='%s')\" % (self.alphabet, self.fruit, self.country,self.nationality)\n",
    "\n",
    "\n",
    "def apply_transforms(fruits, countries,nationalities):\n",
    "    def map_to_alphabet_kv(word):\n",
    "        return (word[0], word)\n",
    "\n",
    "    def cogbk_result_to_wordsalphabet(cgbk_result):\n",
    "        (alphabet, words) = cgbk_result\n",
    "        return WordsAlphabet(alphabet, words['fruits'][0], words['countries'][0],words['nationalities'][0])\n",
    "\n",
    "    fruits_kv = (fruits | 'Fruit to KV' >> beam.Map(map_to_alphabet_kv))\n",
    "    countries_kv = (countries | 'Country to KV' >> beam.Map(map_to_alphabet_kv))\n",
    "    nationalities_kv = (nationalities | 'nationality to KV' >> beam.Map(map_to_alphabet_kv))\n",
    "    return ({'fruits': fruits_kv, 'countries': countries_kv,'nationalities': nationalities_kv}\n",
    "            | beam.CoGroupByKey()\n",
    "            | beam.Map(cogbk_result_to_wordsalphabet))\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "    fruits = p | 'Fruits' >> beam.Create(['apple', 'banana', 'cherry','doctor'])\n",
    "    countries = p | 'Countries' >> beam.Create(['australia', 'brazil', 'canada','duchland'])\n",
    "    nationalities = p | 'Nationalities' >> beam.Create(['australian', 'brazilian', 'canadian','duch'])\n",
    "\n",
    "    (apply_transforms(fruits, countries,nationalities)\n",
    "    | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddda3281-6f17-4af7-97de-f80a349af064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "example_list=[10, 20, 50, 70, 90]\n",
    "\n",
    "\n",
    "class AverageFn(beam.CombineFn):\n",
    "\n",
    "    def create_accumulator(self):\n",
    "        return 0.0, 0\n",
    "\n",
    "    def add_input(self, accumulator, element):\n",
    "        (sum, i) = accumulator\n",
    "        # print(accumulator)\n",
    "        # print(element)\n",
    "        return sum + element, i + 1\n",
    "\n",
    "    def merge_accumulators(self, accumulators):\n",
    "        sums, counts = zip(*accumulators)\n",
    "        print(accumulators)\n",
    "        return sum(sums), sum(counts)\n",
    "\n",
    "    def extract_output(self, accumulator):\n",
    "        (sum, count) = accumulator\n",
    "        return sum / count if count else float('NaN')\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    clear_output()\n",
    "    (p | beam.Create(example_list)\n",
    "     | beam.CombineGlobally(AverageFn())\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*\n",
    "np.mean(example_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9489291-aa53-437b-96c9-d8cf10dd9a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.5\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as p:\n",
    "  (p | beam.Create(range(1, 11))\n",
    "     | beam.combiners.Mean.Globally()\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aadeb62d-2ae1-4b2e-8bf3-ced76462b771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.0\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as p:\n",
    "  (p | beam.Create(example_list)\n",
    "     | beam.combiners.Mean.Globally()\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3f9d31e-5614-4702-8a94-df863f644c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as p:\n",
    "  (p | beam.Create(range(1, 11))\n",
    "     | beam.combiners.Count.Globally()\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6a84751-776d-4a45-bd4c-3785508b90d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as p:\n",
    "  (p | beam.Create(range(1, 11))\n",
    "     | beam.combiners.Latest.Globally()\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb0e74c5-1575-459d-996c-cc08267c2822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as p:\n",
    "  (p | beam.Create(range(1, 11))\n",
    "     | beam.combiners.ToList()\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc6f0a79-190c-4170-bbba-ab6206370351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event(1, book-order_1, 2020-03-01 01:19:00+00:00)\n",
      "Event(2, book-order_2, 2020-03-02 02:18:00+00:00)\n",
      "Event(3, book-order_3, 2020-03-03 03:17:00+00:00)\n",
      "Event(4, book-order_4, 2020-03-04 04:16:00+00:00)\n",
      "Event(5, book-order_5, 2020-03-05 05:15:00+00:00)\n",
      "Event(6, book-order_6, 2020-03-06 06:14:00+00:00)\n",
      "Event(7, book-order_7, 2020-03-07 07:13:00+00:00)\n",
      "Event(8, book-order_8, 2020-03-08 08:12:00+00:00)\n",
      "Event(9, book-order_9, 2020-03-09 09:11:00+00:00)\n",
      "Event(10, book-order_10, 2020-03-10 10:10:00+00:00)\n",
      "Event(11, book-order_11, 2020-03-11 11:09:00+00:00)\n",
      "Event(12, book-order_12, 2020-03-12 12:08:00+00:00)\n",
      "Event(13, book-order_13, 2020-03-13 13:07:00+00:00)\n",
      "Event(14, book-order_14, 2020-03-14 14:06:00+00:00)\n",
      "Event(15, book-order_15, 2020-03-15 15:05:00+00:00)\n",
      "Event(16, book-order_16, 2020-03-16 16:04:00+00:00)\n",
      "Event(17, book-order_17, 2020-03-17 17:03:00+00:00)\n",
      "Event(18, book-order_18, 2020-03-18 18:02:00+00:00)\n"
     ]
    }
   ],
   "source": [
    "#   Licensed to the Apache Software Foundation (ASF) under one\n",
    "#   or more contributor license agreements.  See the NOTICE file\n",
    "#   distributed with this work for additional information\n",
    "#   regarding copyright ownership.  The ASF licenses this file\n",
    "#   to you under the Apache License, Version 2.0 (the\n",
    "#   \"License\"); you may not use this file except in compliance\n",
    "#   with the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#       http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#   Unless required by applicable law or agreed to in writing, software\n",
    "#   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#   See the License for the specific language governing permissions and\n",
    "#   limitations under the License.\n",
    "\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.transforms import window\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Event:\n",
    "    def __init__(self, id, event, timestamp):\n",
    "        self.id = id\n",
    "        self.event = event\n",
    "        self.timestamp = timestamp\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'Event({self.id}, {self.event}, {self.timestamp})'\n",
    "\n",
    "\n",
    "class AddTimestampDoFn(beam.DoFn):\n",
    "\n",
    "    def process(self, element, **kwargs):\n",
    "        unix_timestamp = element.timestamp.timestamp()\n",
    "        yield window.TimestampedValue(element, unix_timestamp)\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "  (p | beam.Create(\n",
    "      [Event(str(x), f'book-order_{x}', datetime.datetime(2020, 3, x, x, 20-x, 0, 0, tzinfo=pytz.UTC)) \\\n",
    "       for x in range(1,19) ] \n",
    "  )\n",
    "     | beam.ParDo(AddTimestampDoFn())\n",
    "    | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e1ed8fb-4670-42d3-9a15-77d16abed394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile  wordcount.py\n",
    "#\n",
    "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
    "# contributor license agreements.  See the NOTICE file distributed with\n",
    "# this work for additional information regarding copyright ownership.\n",
    "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
    "# (the \"License\"); you may not use this file except in compliance with\n",
    "# the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "\"\"\"A word-counting workflow.\"\"\"\n",
    "\n",
    "# pytype: skip-file\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "\n",
    "class WordExtractingDoFn(beam.DoFn):\n",
    "  \"\"\"Parse each line of input text into words.\"\"\"\n",
    "  def process(self, element):\n",
    "    \"\"\"Returns an iterator over the words of this element.\n",
    "\n",
    "    The element is a line of text.  If the line is blank, note that, too.\n",
    "\n",
    "    Args:\n",
    "      element: the element being processed\n",
    "\n",
    "    Returns:\n",
    "      The processed element.\n",
    "    \"\"\"\n",
    "    # return re.findall(r'[\\w\\']+', element, re.UNICODE)\n",
    "    return element.split()\n",
    "    \n",
    "\n",
    "\n",
    "def run(argv=None, save_main_session=True):\n",
    "  \"\"\"Main entry point; defines and runs the wordcount pipeline.\"\"\"\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--input',\n",
    "      dest='input',\n",
    "      default='gs://dataflow-samples/shakespeare/kinglear.txt',\n",
    "      \n",
    "      help='Input file to process.')\n",
    "  parser.add_argument(\n",
    "      '--output',\n",
    "      dest='output',\n",
    "      required=True,\n",
    "      help='Output file to write results to.')\n",
    "  known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "\n",
    "  # We use the save_main_session option because one or more DoFn's in this\n",
    "  # workflow rely on global context (e.g., a module imported at module level).\n",
    "  pipeline_options = PipelineOptions(pipeline_args)\n",
    "  pipeline_options.view_as(SetupOptions).save_main_session = save_main_session\n",
    "\n",
    "  # The pipeline will be run on exiting the with block.\n",
    "  with beam.Pipeline(options=pipeline_options) as p:\n",
    "\n",
    "    # Read the text file[pattern] into a PCollection.\n",
    "    lines = p | 'Read' >> ReadFromText(known_args.input)\n",
    "\n",
    "    counts = (\n",
    "        lines\n",
    "        | 'Split' >> (beam.ParDo(WordExtractingDoFn()).with_output_types(str))\n",
    "        | 'PairWithOne' >> beam.Map(lambda x: (x, 1))\n",
    "        | 'GroupAndSum' >> beam.CombinePerKey(sum))\n",
    "\n",
    "    # Format the counts into a PCollection of strings.\n",
    "    def format_result(word, count):\n",
    "      return '%s: %d' % (word, count)\n",
    "\n",
    "    output = counts | 'Format' >> beam.MapTuple(format_result)\n",
    "\n",
    "    # Write the output using a \"Write\" transform that has side effects.\n",
    "    # pylint: disable=expression-not-assigned\n",
    "    output | 'Write' >> WriteToText(known_args.output)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  logging.getLogger().setLevel(logging.INFO)\n",
    "  run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5da91863-ab00-4e9b-a904-6d646eba7fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KING: 242\n",
      "LEAR: 222\n",
      "DRAMATIS: 1\n",
      "PERSONAE: 1\n",
      "king: 29\n"
     ]
    }
   ],
   "source": [
    "!python3 wordcount.py --output wordcount_output\n",
    "clear_output()\n",
    "!head -n 5 wordcount_output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43bcba77-32df-43b4-a13f-d5b9de946748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount_with_metric.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount_with_metric.py\n",
    "#\n",
    "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
    "# contributor license agreements.  See the NOTICE file distributed with\n",
    "# this work for additional information regarding copyright ownership.\n",
    "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
    "# (the \"License\"); you may not use this file except in compliance with\n",
    "# the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "\"\"\"A word-counting workflow.\"\"\"\n",
    "\n",
    "# pytype: skip-file\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.metrics import Metrics\n",
    "from apache_beam.metrics.metric import MetricsFilter\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "\n",
    "class WordExtractingDoFn(beam.DoFn):\n",
    "  \"\"\"Parse each line of input text into words.\"\"\"\n",
    "  def __init__(self):\n",
    "    # TODO(BEAM-6158): Revert the workaround once we can pickle super() on py3.\n",
    "    # super().__init__()\n",
    "    beam.DoFn.__init__(self)\n",
    "    self.words_counter = Metrics.counter(self.__class__, 'words')\n",
    "    self.word_lengths_counter = Metrics.counter(self.__class__, 'word_lengths')\n",
    "    self.word_lengths_dist = Metrics.distribution(\n",
    "        self.__class__, 'word_len_dist')\n",
    "    self.empty_line_counter = Metrics.counter(self.__class__, 'empty_lines')\n",
    "\n",
    "  def process(self, element):\n",
    "    \"\"\"Returns an iterator over the words of this element.\n",
    "\n",
    "    The element is a line of text.  If the line is blank, note that, too.\n",
    "\n",
    "    Args:\n",
    "      element: the element being processed\n",
    "\n",
    "    Returns:\n",
    "      The processed element.\n",
    "    \"\"\"\n",
    "    text_line = element.strip()\n",
    "    if not text_line:\n",
    "      self.empty_line_counter.inc(1)\n",
    "    words = re.findall(r'[\\w\\']+', text_line, re.UNICODE)\n",
    "    for w in words:\n",
    "      self.words_counter.inc()\n",
    "      self.word_lengths_counter.inc(len(w))\n",
    "      self.word_lengths_dist.update(len(w))\n",
    "    return words\n",
    "\n",
    "\n",
    "def main(argv=None, save_main_session=True):\n",
    "  \"\"\"Main entry point; defines and runs the wordcount pipeline.\"\"\"\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--input',\n",
    "      dest='input',\n",
    "      default='gs://dataflow-samples/shakespeare/kinglear.txt',\n",
    "      help='Input file to process.')\n",
    "  parser.add_argument(\n",
    "      '--output',\n",
    "      dest='output',\n",
    "      required=True,\n",
    "      help='Output file to write results to.')\n",
    "  known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "\n",
    "  # We use the save_main_session option because one or more DoFn's in this\n",
    "  # workflow rely on global context (e.g., a module imported at module level).\n",
    "  pipeline_options = PipelineOptions(pipeline_args)\n",
    "  pipeline_options.view_as(SetupOptions).save_main_session = save_main_session\n",
    "  p = beam.Pipeline(options=pipeline_options)\n",
    "\n",
    "  # Read the text file[pattern] into a PCollection.\n",
    "  lines = p | 'read' >> ReadFromText(known_args.input)\n",
    "\n",
    "  # Count the occurrences of each word.\n",
    "  def count_ones(word_ones):\n",
    "    (word, ones) = word_ones\n",
    "    return (word, sum(ones))\n",
    "\n",
    "  counts = (\n",
    "      lines\n",
    "      | 'split' >> (beam.ParDo(WordExtractingDoFn()).with_output_types(str))\n",
    "      | 'pair_with_one' >> beam.Map(lambda x: (x, 1))\n",
    "      | 'group' >> beam.GroupByKey()\n",
    "      | 'count' >> beam.Map(count_ones))\n",
    "\n",
    "  # Format the counts into a PCollection of strings.\n",
    "  def format_result(word_count):\n",
    "    (word, count) = word_count\n",
    "    return '%s: %d' % (word, count)\n",
    "\n",
    "  output = counts | 'format' >> beam.Map(format_result)\n",
    "\n",
    "  # Write the output using a \"Write\" transform that has side effects.\n",
    "  # pylint: disable=expression-not-assigned\n",
    "  output | 'write' >> WriteToText(known_args.output)\n",
    "\n",
    "  result = p.run()\n",
    "  result.wait_until_finish()\n",
    "\n",
    "  # Do not query metrics when creating a template which doesn't run\n",
    "  if (not hasattr(result, 'has_job')  # direct runner\n",
    "      or result.has_job):  # not just a template creation\n",
    "    empty_lines_filter = MetricsFilter().with_name('empty_lines')\n",
    "    query_result = result.metrics().query(empty_lines_filter)\n",
    "    if query_result['counters']:\n",
    "      empty_lines_counter = query_result['counters'][0]\n",
    "      logging.info('number of empty lines: %d', empty_lines_counter.result)\n",
    "\n",
    "    word_lengths_filter = MetricsFilter().with_name('word_len_dist')\n",
    "    query_result = result.metrics().query(word_lengths_filter)\n",
    "    if query_result['distributions']:\n",
    "      word_lengths_dist = query_result['distributions'][0]\n",
    "      logging.info('average word length: %d', word_lengths_dist.result.mean)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  logging.getLogger().setLevel(logging.INFO)\n",
    "  main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4451586e-5ccd-4358-85ad-dc51a0dc15df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KING: 243\n",
      "LEAR: 236\n",
      "DRAMATIS: 1\n",
      "PERSONAE: 1\n",
      "king: 65\n"
     ]
    }
   ],
   "source": [
    "!python3 wordcount_with_metric.py --output wordcount_output2\n",
    "clear_output()\n",
    "!head -n 5 wordcount_output2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92f8dea4-81d8-4d8b-ab15-b26a1b2563b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wordcounttest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcounttest.py\n",
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
    "# contributor license agreements.  See the NOTICE file distributed with\n",
    "# this work for additional information regarding copyright ownership.\n",
    "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
    "# (the \"License\"); you may not use this file except in compliance with\n",
    "# the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "\"\"\"Test for the wordcount example.\"\"\"\n",
    "\n",
    "# pytype: skip-file\n",
    "\n",
    "import collections\n",
    "import logging\n",
    "import re\n",
    "import tempfile\n",
    "import unittest\n",
    "\n",
    "import pytest\n",
    "\n",
    "from apache_beam.examples import wordcount\n",
    "from apache_beam.testing.util import open_shards\n",
    "\n",
    "\n",
    "@pytest.mark.examples_postcommit\n",
    "class WordCountTest(unittest.TestCase):\n",
    "\n",
    "  SAMPLE_TEXT = (\n",
    "      u'a b c a b a\\nacento gr√°fico\\nJuly 30, 2018\\n\\n aa bb cc aa bb aa')\n",
    "\n",
    "  def create_temp_file(self, contents):\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "      f.write(contents.encode('utf-8'))\n",
    "      return f.name\n",
    "\n",
    "  def test_basics(self):\n",
    "    temp_path = self.create_temp_file(self.SAMPLE_TEXT)\n",
    "    expected_words = collections.defaultdict(int)\n",
    "    for word in re.findall(r'[\\w\\']+', self.SAMPLE_TEXT, re.UNICODE):\n",
    "      expected_words[word] += 1\n",
    "    wordcount.run(['--input=%s*' % temp_path, '--output=%s.result' % temp_path],\n",
    "                  save_main_session=False)\n",
    "    # Parse result file and compare.\n",
    "    results = []\n",
    "    with open_shards(temp_path + '.result-*-of-*') as result_file:\n",
    "      for line in result_file:\n",
    "        match = re.search(r'(\\S+): ([0-9]+)', line)\n",
    "        if match is not None:\n",
    "          results.append((match.group(1), int(match.group(2))))\n",
    "    self.assertEqual(sorted(results), sorted(expected_words.items()))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  logging.getLogger().setLevel(logging.INFO)\n",
    "  unittest.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05a5b89f-9a6e-47fa-a562-dcca2451e03e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.43.0\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f9817e52af0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f9817e52c10> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f9817e54160> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f9817e541f0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f9817e543a0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f9817e54430> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f9817e54550> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f9817e545e0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f9817e54670> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f9817e54700> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f9817e54940> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function add_impulse_to_dangling_transforms at 0x7f9817e54a60> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f9817e548b0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f9817e549d0> ====================\n",
      "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 104857600\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f9817df1f70> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.00 seconds.\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.561s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 wordcounttest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95368eba-ef67-4b70-b57c-bab0e0562f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', ['apple', 'ant'])\n",
      "('b', ['ball', 'bear'])\n",
      "('c', ['car', 'cheetah'])\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "  (p | beam.Create(['apple', 'ball', 'car', 'bear', 'cheetah', 'ant'])\n",
    "     | beam.Map(lambda word: (word[0], word))\n",
    "     | beam.GroupByKey()\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e7fc12b-a715-410f-9e23-e966a2aec1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordsAlphabet(alphabet:'a', fruit='apple', country='australia')\n",
      "WordsAlphabet(alphabet:'b', fruit='banana', country='brazil')\n",
      "WordsAlphabet(alphabet:'c', fruit='cherry', country='canada')\n"
     ]
    }
   ],
   "source": [
    "#   Licensed to the Apache Software Foundation (ASF) under one\n",
    "#   or more contributor license agreements.  See the NOTICE file\n",
    "#   distributed with this work for additional information\n",
    "#   regarding copyright ownership.  The ASF licenses this file\n",
    "#   to you under the Apache License, Version 2.0 (the\n",
    "#   \"License\"); you may not use this file except in compliance\n",
    "#   with the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#       http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#   Unless required by applicable law or agreed to in writing, software\n",
    "#   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#   See the License for the specific language governing permissions and\n",
    "#   limitations under the License.\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "class WordsAlphabet:\n",
    "\n",
    "    def __init__(self, alphabet, fruit, country):\n",
    "        self.alphabet = alphabet\n",
    "        self.fruit = fruit\n",
    "        self.country = country\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"WordsAlphabet(alphabet:'%s', fruit='%s', country='%s')\" % (self.alphabet, self.fruit, self.country)\n",
    "\n",
    "\n",
    "def apply_transforms(fruits, countries):\n",
    "    def map_to_alphabet_kv(word):\n",
    "        return (word[0], word)\n",
    "\n",
    "    def cogbk_result_to_wordsalphabet(cgbk_result):\n",
    "        (alphabet, words) = cgbk_result\n",
    "        return WordsAlphabet(alphabet, words['fruits'][0], words['countries'][0])\n",
    "\n",
    "    fruits_kv = (fruits | 'Fruit to KV' >> beam.Map(map_to_alphabet_kv))\n",
    "    countries_kv = (countries | 'Country to KV' >> beam.Map(map_to_alphabet_kv))\n",
    "\n",
    "    return ({'fruits': fruits_kv, 'countries': countries_kv}\n",
    "            | beam.CoGroupByKey()\n",
    "            | beam.Map(cogbk_result_to_wordsalphabet))\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "  fruits = p | 'Fruits' >> beam.Create(['apple', 'banana', 'cherry'])\n",
    "  countries = p | 'Countries' >> beam.Create(['australia', 'brazil', 'canada'])\n",
    "\n",
    "  (apply_transforms(fruits, countries)\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b115eb3-ee0a-4373-ad64-6179c81a8ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Player 1', 115)\n",
      "('Player 2', 85)\n",
      "('Player 3', 25)\n"
     ]
    }
   ],
   "source": [
    "#combine per key\n",
    "import apache_beam as beam\n",
    "\n",
    "PLAYER_1 = 'Player 1'\n",
    "PLAYER_2 = 'Player 2'\n",
    "PLAYER_3 = 'Player 3'\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "  (p | beam.Create([(PLAYER_1, 15), (PLAYER_2, 10), (PLAYER_1, 100),\n",
    "                    (PLAYER_3, 25), (PLAYER_2, 75)])\n",
    "     | beam.CombinePerKey(sum)\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e040642-f2e7-448c-bd4d-801cab2b4864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "#combine simple function\n",
    "import apache_beam as beam\n",
    "\n",
    "def sum(numbers):\n",
    "    total = 0\n",
    "\n",
    "    for num in numbers:\n",
    "        total += num\n",
    "\n",
    "    return total\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "\n",
    "  (p | beam.Create([1, 2, 3, 4, 5])\n",
    "     | beam.CombineGlobally(sum)\n",
    "     | beam.io.textio.WriteToText('example-output'))\n",
    "clear_output()\n",
    "!cat example-output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c49e22a-a224-49fb-b1a8-5ed7fbc988b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
