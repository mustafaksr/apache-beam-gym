{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32567b01-4f36-481a-bb14-6a1cec28b9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lab8_my_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lab8_my_pipeline.py\n",
    "import argparse\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.runners import DataflowRunner, DirectRunner\n",
    "\n",
    "# ### functions and classes\n",
    "\n",
    "def parse_json(element):\n",
    "    return json.loads(element)\n",
    "\n",
    "def drop_fields(element):\n",
    "    element.pop('user_agent')\n",
    "    return element\n",
    "\n",
    "# ### main\n",
    "\n",
    "def run():\n",
    "    # Command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Load from Json into BigQuery')\n",
    "    parser.add_argument('--project',required=True, help='Specify Google Cloud project')\n",
    "    parser.add_argument('--region', required=True, help='Specify Google Cloud region')\n",
    "    parser.add_argument('--runner', required=True, help='Specify Apache Beam Runner')\n",
    "    parser.add_argument('--inputPath', required=True, help='Path to events.json')\n",
    "    parser.add_argument('--outputPath', required=True, help='Path to coldline storage bucket')\n",
    "    parser.add_argument('--tableName', required=True, help='BigQuery table name')\n",
    "\n",
    "    opts, pipeline_opts = parser.parse_known_args()\n",
    "\n",
    "    # Setting up the Beam pipeline options\n",
    "    options = PipelineOptions(pipeline_opts)\n",
    "    options.view_as(GoogleCloudOptions).project = opts.project\n",
    "    options.view_as(GoogleCloudOptions).region = opts.region\n",
    "    options.view_as(GoogleCloudOptions).job_name = '{0}{1}'.format('my-pipeline-',time.time_ns())\n",
    "    options.view_as(StandardOptions).runner = opts.runner\n",
    "\n",
    "    input_path = opts.inputPath\n",
    "    output_path = opts.outputPath\n",
    "    table_name = opts.tableName\n",
    "\n",
    "    # Table schema for BigQuery\n",
    "    table_schema = {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"ip\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"user_id\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"lat\",\n",
    "                \"type\": \"FLOAT\",\n",
    "                \"mode\": \"NULLABLE\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"lng\",\n",
    "                \"type\": \"FLOAT\",\n",
    "                \"mode\": \"NULLABLE\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"timestamp\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"http_request\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"http_response\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"num_bytes\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create the pipeline\n",
    "    p = beam.Pipeline(options=options)\n",
    "\n",
    "    '''\n",
    "\n",
    "    Steps:\n",
    "    1) Read something\n",
    "    2) Transform something\n",
    "    3) Write something\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Read in lines to an initial PCollection that can then be branched off of\n",
    "    lines = p | 'ReadFromGCS' >> beam.io.ReadFromText(input_path)\n",
    "\n",
    "    # Write to Google Cloud Storage\n",
    "    lines | 'WriteRawToGCS' >> beam.io.WriteToText(output_path)\n",
    "\n",
    "    # Read elements from Json, filter out individual elements, and write to BigQuery\n",
    "    (lines\n",
    "        | 'ParseJson' >> beam.Map(parse_json)\n",
    "        | 'DropFields' >> beam.Map(drop_fields)\n",
    "        | 'FilterFn' >> beam.Filter(lambda row: row['num_bytes'] < 120)\n",
    "        | 'WriteToBQ' >> beam.io.WriteToBigQuery(\n",
    "            table_name,\n",
    "            schema=table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE\n",
    "            )\n",
    "    )\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Building pipeline ...\")\n",
    "\n",
    "    p.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6065a06-87ed-4560-880f-8efd3ebbf70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1d4f6a1-a3aa-4b70-8553-eb8b872cef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"workdir\"]=\"/path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0e8ab01-893f-4e17-aca8-2b485e53547f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/#!/usr/bin/env bash\n",
      "echo \"Creating pipeline sinks\"\n",
      "\n",
      "PROJECT_ID=$(gcloud config get-value project)\n",
      "\n",
      "# GCS buckets\n",
      "#TODO: Add try/catch for the first bucket since qwiklabs\n",
      "gsutil mb -l US gs://$PROJECT_ID\n",
      "gsutil mb -l US -c \"COLDLINE\" gs://$PROJECT_ID-coldline\n",
      "\n",
      "# BiqQuery Dataset\n",
      "bq mk --location=US logs"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat $workdir/create_batch_sinks.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63383d04-1b4b-4cdf-91b1-8fd4ebb03ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/#!/usr/bin/env bash\n",
      "echo \"Installing packages\"\n",
      "# Install modules\n",
      "sh ./install_packages.sh\n",
      "\n",
      "echo \"Generating synthetic users\"\n",
      "# Generate 2 fake web site users\n",
      "python3 user_generator.py --n=10\n",
      "\n",
      "echo \"Generating synthetic events\"\n",
      "rm *.out 2> /dev/null\n",
      "# Generate 10 events\n",
      "python3 batch_event_generator.py --num_e=1000\n",
      "\n",
      "echo \"Copying events to Cloud Storage\"\n",
      "# Set BUCKET to the non-coldline Google Cloud Storage bucket\n",
      "export BUCKET=gs://$(gcloud config get-value project)/\n",
      "# Copy events.json into the bucket\n",
      "gsutil cp events.json ${BUCKET}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat $workdir/generate_batch_events.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76288133-d1fa-47bc-877a-96ef1d050a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash $workdir/create_batch_sinks.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120316c7-097a-410a-a925-12546878ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash $workdir/generate_batch_events.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78472401-d03c-49f3-988e-2969b24413ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f3e38b-7613-4fac-bb52-c6e642b05751",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT_ID\"]=PROJECT_ID\n",
    "os.environ[\"REGION\"]='us-central1'\n",
    "os.environ[\"BUCKET\"]=\"gs://\"+PROJECT_ID\n",
    "os.environ[\"COLDLINE_BUCKET\"]=\"gs://\"+PROJECT_ID+\"-coldline\"\n",
    "os.environ[\"PIPELINE_FOLDER\"]=\"gs://\"+PROJECT_ID\n",
    "os.environ[\"INPUT_PATH\"]=\"gs://\"+PROJECT_ID+\"/events.json\"\n",
    "os.environ[\"OUTPUT_PATH\"]=\"gs://\"+PROJECT_ID+\"-coldline/pipeline_output\"\n",
    "os.environ[\"TABLE_NAME\"]=PROJECT_ID+\":logs.logs_filtered\"\n",
    "os.environ[\"PUBSUB_TOPIC\"]=f\"projects/{PROJECT_ID}/topics/my_topic\"\n",
    "os.environ[\"WINDOW_DURATION\"]=60\n",
    "os.environ[\"AGGREGATE_TABLE_NAME\"]=f\"{PROJECT_ID}:logs.windowed_traffic\"\n",
    "os.environ[\"RAW_TABLE_NAME\"]=f\"{PROJECT_ID}:logs.raw\"\n",
    "os.environ[\"RUNNER\"]=\"DataflowRunner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa0b5e5-76aa-47e2-b3e2-b9a39170d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment variables\n",
    "# export PROJECT_ID=$(gcloud config get-value project)\n",
    "# export REGION='us-central1'\n",
    "# export BUCKET=gs://${PROJECT_ID}\n",
    "# export COLDLINE_BUCKET=${BUCKET}-coldline\n",
    "# export PIPELINE_FOLDER=${BUCKET}\n",
    "# export RUNNER=DataflowRunner\n",
    "# export INPUT_PATH=${PIPELINE_FOLDER}/events.json\n",
    "# export OUTPUT_PATH=${PIPELINE_FOLDER}-coldline/pipeline_output\n",
    "# export TABLE_NAME=${PROJECT_ID}:logs.logs_filtered\n",
    "python3 lab8_my_pipeline.py \\\n",
    "--project=${PROJECT_ID} \\\n",
    "--region=${REGION} \\\n",
    "--stagingLocation=${PIPELINE_FOLDER}/staging \\\n",
    "--tempLocation=${PIPELINE_FOLDER}/temp \\\n",
    "--runner=${RUNNER} \\\n",
    "--inputPath=${INPUT_PATH} \\\n",
    "--outputPath=${OUTPUT_PATH} \\\n",
    "--tableName=${TABLE_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21751d7a-40a6-492b-a8a6-aa0cf139e2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/#!/usr/bin/env bash\n",
      "echo \"Installing packages\"\n",
      "# Install modules\n",
      "sh ./install_packages.sh\n",
      "\n",
      "echo \"Generating synthetic users\"\n",
      "# Generate 10 fake web site users\n",
      "python3 user_generator.py --n=10\n",
      "\n",
      "echo \"Generating synthetic events\"\n",
      "use_lag=$1\n",
      "\n",
      "if [ \"$use_lag\" = true ] ; then\n",
      "    echo \"Using lag\"\n",
      "    python3 streaming_event_generator.py --project_id=$(gcloud config get-value project) -t=my_topic\n",
      "else\n",
      "    echo \"Not using lag\"\n",
      "    python3 streaming_event_generator.py --project_id=$(gcloud config get-value project) -t=my_topic -off=1. -on=0. -l=0\n",
      "fi"
     ]
    }
   ],
   "source": [
    "cat $workdir/generate_streaming_events.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8128bc6c-ae2b-462e-be24-f7ccceeae5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bash $workdir/generate_streaming_events.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
