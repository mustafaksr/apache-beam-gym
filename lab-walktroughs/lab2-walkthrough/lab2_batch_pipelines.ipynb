{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4713828-06a0-4257-b48f-a746f9324d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lab2_batch_minute_traffic_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lab2_batch_minute_traffic_pipeline.py\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import typing\n",
    "from datetime import datetime\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.transforms.combiners import CountCombineFn\n",
    "from apache_beam.runners import DataflowRunner, DirectRunner\n",
    "\n",
    "# ### functions and classes\n",
    "\n",
    "class CommonLog(typing.NamedTuple):\n",
    "    ip: str\n",
    "    user_id: str\n",
    "    lat: float\n",
    "    lng: float\n",
    "    timestamp: str\n",
    "    http_request: str\n",
    "    http_response: int\n",
    "    num_bytes: int\n",
    "    user_agent: str\n",
    "\n",
    "beam.coders.registry.register_coder(CommonLog, beam.coders.RowCoder)\n",
    "\n",
    "def parse_json(element):\n",
    "    row = json.loads(element)\n",
    "    return CommonLog(**row)\n",
    "\n",
    "def add_timestamp(element):\n",
    "    ts = datetime.strptime(element.timestamp[:-8], \"%Y-%m-%dT%H:%M:%S\").timestamp()\n",
    "    return beam.window.TimestampedValue(element, ts)\n",
    "\n",
    "class GetTimestampFn(beam.DoFn):\n",
    "    def process(self, element, window=beam.DoFn.WindowParam):\n",
    "        window_start = window.start.to_utc_datetime().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        output = {'page_views': element, 'timestamp': window_start}\n",
    "        yield output\n",
    "\n",
    "# ### main\n",
    "\n",
    "def run():\n",
    "    # Command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Load from Json into BigQuery')\n",
    "    parser.add_argument('--project',required=True, help='Specify Google Cloud project')\n",
    "    parser.add_argument('--region', required=True, help='Specify Google Cloud region')\n",
    "    parser.add_argument('--staging_location', required=True, help='Specify Cloud Storage bucket for staging')\n",
    "    parser.add_argument('--temp_location', required=True, help='Specify Cloud Storage bucket for temp')\n",
    "    parser.add_argument('--runner', required=True, help='Specify Apache Beam Runner')\n",
    "    parser.add_argument('--input_path', required=True, help='Path to events.json')\n",
    "    parser.add_argument('--table_name', required=True, help='BigQuery table name')\n",
    "\n",
    "    opts = parser.parse_args()\n",
    "\n",
    "    # Setting up the Beam pipeline options\n",
    "    options = PipelineOptions(save_main_session=True)\n",
    "    options.view_as(GoogleCloudOptions).project = opts.project\n",
    "    options.view_as(GoogleCloudOptions).region = opts.region\n",
    "    options.view_as(GoogleCloudOptions).staging_location = opts.staging_location\n",
    "    options.view_as(GoogleCloudOptions).temp_location = opts.temp_location\n",
    "    options.view_as(GoogleCloudOptions).job_name = '{0}{1}'.format('batch-minute-traffic-pipeline-',time.time_ns())\n",
    "    options.view_as(StandardOptions).runner = opts.runner\n",
    "\n",
    "    input_path = opts.input_path\n",
    "    table_name = opts.table_name\n",
    "\n",
    "    # Table schema for BigQuery\n",
    "    table_schema = {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"page_views\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"timestamp\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create the pipeline\n",
    "    p = beam.Pipeline(options=options)\n",
    "\n",
    "\n",
    "\n",
    "    (p | 'ReadFromGCS' >> beam.io.ReadFromText(input_path)\n",
    "       | 'ParseJson' >> beam.Map(parse_json).with_output_types(CommonLog)\n",
    "       | 'AddEventTimestamp' >> beam.Map(add_timestamp)\n",
    "       | \"WindowByMinute\" >> beam.WindowInto(beam.window.FixedWindows(60))\n",
    "       | \"CountPerMinute\" >> beam.CombineGlobally(CountCombineFn()).without_defaults()\n",
    "       | \"AddWindowTimestamp\" >> beam.ParDo(GetTimestampFn())\n",
    "       | 'WriteToBQ' >> beam.io.WriteToBigQuery(\n",
    "            table_name,\n",
    "            schema=table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE\n",
    "            )\n",
    "    )\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Building pipeline ...\")\n",
    "\n",
    "    p.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94286ef2-cf71-4452-bbf2-b24b3429ac10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lab2_batch_user_traffic_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lab2_batch_user_traffic_pipeline.py\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import typing\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.transforms.combiners import CountCombineFn\n",
    "from apache_beam.runners import DataflowRunner, DirectRunner\n",
    "\n",
    "# ### functions and classes\n",
    "\n",
    "class CommonLog (typing.NamedTuple):\n",
    "    ip: str\n",
    "    user_id: str\n",
    "    lat: float\n",
    "    lng: float\n",
    "    timestamp: str\n",
    "    http_request: str\n",
    "    http_response: int\n",
    "    num_bytes: int\n",
    "    user_agent: str\n",
    "\n",
    "class PerUserAggregation(typing.NamedTuple):\n",
    "    user_id: str\n",
    "    page_views: int\n",
    "    total_bytes: int\n",
    "    max_bytes: int\n",
    "    min_bytes: int\n",
    "\n",
    "beam.coders.registry.register_coder(CommonLog, beam.coders.RowCoder)\n",
    "beam.coders.registry.register_coder(PerUserAggregation, beam.coders.RowCoder)\n",
    "\n",
    "def parse_json(element):\n",
    "    row = json.loads(element)\n",
    "    return CommonLog(**row)\n",
    "\n",
    "def to_dict(element):\n",
    "    return element._asdict()\n",
    "\n",
    "\n",
    "# ### main\n",
    "\n",
    "def run():\n",
    "    # Command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Load from Json into BigQuery')\n",
    "    parser.add_argument('--project',required=True, help='Specify Google Cloud project')\n",
    "    parser.add_argument('--region', required=True, help='Specify Google Cloud region')\n",
    "    parser.add_argument('--staging_location', required=True, help='Specify Cloud Storage bucket for staging')\n",
    "    parser.add_argument('--temp_location', required=True, help='Specify Cloud Storage bucket for temp')\n",
    "    parser.add_argument('--runner', required=True, help='Specify Apache Beam Runner')\n",
    "    parser.add_argument('--input_path', required=True, help='Path to events.json')\n",
    "    parser.add_argument('--table_name', required=True, help='BigQuery table name')\n",
    "\n",
    "    opts = parser.parse_args()\n",
    "\n",
    "    # Setting up the Beam pipeline options\n",
    "    options = PipelineOptions(save_main_session=True)\n",
    "    options.view_as(GoogleCloudOptions).project = opts.project\n",
    "    options.view_as(GoogleCloudOptions).region = opts.region\n",
    "    options.view_as(GoogleCloudOptions).staging_location = opts.staging_location\n",
    "    options.view_as(GoogleCloudOptions).temp_location = opts.temp_location\n",
    "    options.view_as(GoogleCloudOptions).job_name = '{0}{1}'.format('batch-user-traffic-pipeline-',time.time_ns())\n",
    "    options.view_as(StandardOptions).runner = opts.runner\n",
    "\n",
    "    input_path = opts.input_path\n",
    "    table_name = opts.table_name\n",
    "\n",
    "    # Table schema for BigQuery\n",
    "    table_schema = {\n",
    "        \"fields\": [\n",
    "\n",
    "            {\n",
    "                \"name\": \"user_id\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"page_views\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"total_bytes\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"max_bytes\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"min_bytes\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create the pipeline\n",
    "    p = beam.Pipeline(options=options)\n",
    "\n",
    "\n",
    "\n",
    "    (p | 'ReadFromGCS' >> beam.io.ReadFromText(input_path)\n",
    "       | 'ParseJson' >> beam.Map(parse_json).with_output_types(CommonLog)\n",
    "       | 'PerUserAggregations' >> beam.GroupBy('user_id')\n",
    "                                      .aggregate_field('user_id', CountCombineFn(), 'page_views')\n",
    "                                      .aggregate_field('num_bytes', sum, 'total_bytes')\n",
    "                                      .aggregate_field('num_bytes', max, 'max_bytes')\n",
    "                                      .aggregate_field('num_bytes', min, 'min_bytes')\n",
    "                                      .with_output_types(PerUserAggregation)\n",
    "       | 'ToDict' >> beam.Map(to_dict)\n",
    "       | 'WriteToBQ' >> beam.io.WriteToBigQuery(\n",
    "            table_name,\n",
    "            schema=table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE\n",
    "            )\n",
    "    )\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Building pipeline ...\")\n",
    "\n",
    "    p.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f251c344-800a-4748-9786-0bde24745f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4235fa0d-9aa1-4119-bfe5-3e032f1b65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"workdir\"]=\"/media/desktop/01D7E2330EB2B040/google-cloud-ml/training-data-analyst-master/quests/dataflow_python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "942eb24d-300d-4575-9400-61c62ef6f03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/#!/usr/bin/env bash\n",
      "echo \"Creating pipeline sinks\"\n",
      "\n",
      "PROJECT_ID=$(gcloud config get-value project)\n",
      "\n",
      "# GCS buckets\n",
      "#TODO: Add try/catch for the first bucket since qwiklabs\n",
      "gsutil mb -l US gs://$PROJECT_ID\n",
      "gsutil mb -l US -c \"COLDLINE\" gs://$PROJECT_ID-coldline\n",
      "\n",
      "# BiqQuery Dataset\n",
      "bq mk --location=US logs"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat $workdir/create_batch_sinks.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b0b45f8-811e-4764-a1e5-665bce8cc8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/#!/usr/bin/env bash\n",
      "echo \"Installing packages\"\n",
      "# Install modules\n",
      "sh ./install_packages.sh\n",
      "\n",
      "echo \"Generating synthetic users\"\n",
      "# Generate 2 fake web site users\n",
      "python3 user_generator.py --n=10\n",
      "\n",
      "echo \"Generating synthetic events\"\n",
      "rm *.out 2> /dev/null\n",
      "# Generate 10 events\n",
      "python3 batch_event_generator.py --num_e=1000\n",
      "\n",
      "echo \"Copying events to Cloud Storage\"\n",
      "# Set BUCKET to the non-coldline Google Cloud Storage bucket\n",
      "export BUCKET=gs://$(gcloud config get-value project)/\n",
      "# Copy events.json into the bucket\n",
      "gsutil cp events.json ${BUCKET}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat $workdir/generate_batch_events.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d9af37-dd54-47c5-90ed-142cab149ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash $workdir/create_batch_sinks.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eea8bb-af95-4a3d-b3e6-91cec86f86df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash $workdir/generate_batch_events.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c096e89e-efeb-431d-9bb5-6d050d048952",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19f57607-ec9e-40f3-87e1-547ac4670d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT_ID\"]=PROJECT_ID\n",
    "os.environ[\"REGION\"]='us-central1'\n",
    "os.environ[\"BUCKET\"]=\"gs://\"+PROJECT_ID\n",
    "os.environ[\"PIPELINE_FOLDER\"]=\"gs://\"+PROJECT_ID\n",
    "os.environ[\"INPUT_PATH\"]=\"gs://\"+PROJECT_ID+\"/events.json\"\n",
    "os.environ[\"TABLE_NAME\"]=PROJECT_ID+\":logs.minute_traffic\"\n",
    "os.environ[\"RUNNER\"]=\"DataflowRunner\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b72557-1107-4b21-9c09-a6cdc527a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# export PROJECT_ID=$(gcloud config get-value project)\n",
    "# export REGION='us-central1'\n",
    "# export BUCKET=gs://${PROJECT_ID}\n",
    "# export PIPELINE_FOLDER=${BUCKET}\n",
    "# export RUNNER=DataflowRunner\n",
    "# export INPUT_PATH=${PIPELINE_FOLDER}/events.json\n",
    "# export TABLE_NAME=${PROJECT_ID}:logs.minute_traffic\n",
    "\n",
    "python3 batch_minute_traffic_pipeline.py \\\n",
    "--project=${PROJECT_ID} \\\n",
    "--region=${REGION} \\\n",
    "--staging_location=${PIPELINE_FOLDER}/staging \\\n",
    "--temp_location=${PIPELINE_FOLDER}/temp \\\n",
    "--runner=${RUNNER} \\\n",
    "--input_path=${INPUT_PATH} \\\n",
    "--table_name=${TABLE_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deef75d-cf10-46c0-995b-8ec66173f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TABLE_NAME\"]=PROJECT_ID+\":logs.user_traffic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67bfb0f-5784-48b5-b1b9-1b55c3b97387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export PROJECT_ID=$(gcloud config get-value project)\n",
    "# export REGION='us-central1'\n",
    "# export BUCKET=gs://${PROJECT_ID}\n",
    "# export PIPELINE_FOLDER=${BUCKET}\n",
    "# export RUNNER=DataflowRunner\n",
    "# export INPUT_PATH=${PIPELINE_FOLDER}/events.json\n",
    "# export TABLE_NAME=${PROJECT_ID}:logs.user_traffic\n",
    "\n",
    "python3 batch_user_traffic_pipeline.py \\\n",
    "--project=${PROJECT_ID} \\\n",
    "--region=${REGION} \\\n",
    "--staging_location=${PIPELINE_FOLDER}/staging \\\n",
    "--temp_location=${PIPELINE_FOLDER}/temp \\\n",
    "--runner=${RUNNER} \\\n",
    "--input_path=${INPUT_PATH} \\\n",
    "--table_name=${TABLE_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cacead-5aad-4b8f-bebb-46777f29dd09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c828e84-92e4-4455-afa0-7bdc800ac7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
