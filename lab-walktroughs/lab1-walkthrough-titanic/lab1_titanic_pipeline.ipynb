{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "017684ba-f125-4f29-b056-06ad41ee8aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lab1_titanic_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lab1_titanic_pipeline.py\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.runners import DataflowRunner, DirectRunner\n",
    "\n",
    "# ### main\n",
    "\n",
    "def run():\n",
    "    # Command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Load from Json into BigQuery')\n",
    "    parser.add_argument('--project',required=True, help='Specify Google Cloud project')\n",
    "    parser.add_argument('--region', required=True, help='Specify Google Cloud region')\n",
    "    parser.add_argument('--stagingLocation', required=True, help='Specify Cloud Storage bucket for staging')\n",
    "    parser.add_argument('--tempLocation', required=True, help='Specify Cloud Storage bucket for temp')\n",
    "    parser.add_argument('--runner', required=True, help='Specify Apache Beam Runner')\n",
    "\n",
    "    opts = parser.parse_args()\n",
    "\n",
    "    # Setting up the Beam pipeline options\n",
    "    options = PipelineOptions()\n",
    "    options.view_as(GoogleCloudOptions).project = opts.project\n",
    "    options.view_as(GoogleCloudOptions).region = opts.region\n",
    "    options.view_as(GoogleCloudOptions).staging_location = opts.stagingLocation\n",
    "    options.view_as(GoogleCloudOptions).temp_location = opts.tempLocation\n",
    "    options.view_as(GoogleCloudOptions).job_name = '{0}{1}'.format('my-pipeline-',time.time_ns())\n",
    "    options.view_as(StandardOptions).runner = opts.runner\n",
    "\n",
    "    # Static input and output\n",
    "    input = 'gs://{0}/titanic.json'.format(opts.project)\n",
    "    output = '{0}:logs.logs'.format(opts.project)\n",
    "\n",
    "    # Table schema for BigQuery\n",
    "    table_schema = {\n",
    "        \"fields\": [\n",
    "            { \"name\":\"Survived\" , \"type\":\"INTEGER\" }\n",
    "            { \"name\":\"Pclass  \" , \"type\":\"INTEGER\" }\n",
    "            { \"name\":\"Name    \" , \"type\":\"STRING\" }\n",
    "            { \"name\":\"Sex     \" , \"type\":\"STRING\" }\n",
    "            { \"name\":\"Age     \" , \"type\":\"FLOAT\" }\n",
    "            { \"name\":\"SibSp   \" , \"type\":\"INTEGER\" }\n",
    "            { \"name\":\"Parch   \" , \"type\":\"INTEGER\" }\n",
    "            { \"name\":\"Ticket  \" , \"type\":\"STRING\" }\n",
    "            { \"name\":\"Fare    \" , \"type\":\"FLOAT\" }\n",
    "            { \"name\":\"Cabin   \" , \"type\":\"STRING\" }\n",
    "            { \"name\":\"Embarked\" , \"type\":\"STRING\" }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create the pipeline\n",
    "    p = beam.Pipeline(options=options)\n",
    "\n",
    "    '''\n",
    "\n",
    "    Steps:\n",
    "    1) Read something\n",
    "    2) Transform something\n",
    "    3) Write something\n",
    "\n",
    "    '''\n",
    "\n",
    "    (p\n",
    "        | 'ReadFromGCS' >> beam.io.ReadFromText(input)\n",
    "        | 'ParseJson' >> beam.Map(lambda line: json.loads(line))\n",
    "        | 'WriteToBQ' >> beam.io.WriteToBigQuery(\n",
    "            output,\n",
    "            schema=table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE\n",
    "            )\n",
    "    )\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Building pipeline ...\")\n",
    "\n",
    "    p.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6463904c-f396-4c08-9f74-4169632b42a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c386857d-294f-4665-a8eb-3f3f847e6bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"workdir\"]=\"/media/desktop/01D7E2330EB2B040/google-cloud-ml/training-data-analyst-master/quests/dataflow_python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e9c99bb-f536-4a4d-b2bf-fbfbd1beeec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/#!/usr/bin/env bash\n",
      "echo \"Creating pipeline sinks\"\n",
      "\n",
      "PROJECT_ID=$(gcloud config get-value project)\n",
      "\n",
      "# GCS buckets\n",
      "#TODO: Add try/catch for the first bucket since qwiklabs\n",
      "gsutil mb -l US gs://$PROJECT_ID\n",
      "gsutil mb -l US -c \"COLDLINE\" gs://$PROJECT_ID-coldline\n",
      "\n",
      "# BiqQuery Dataset\n",
      "bq mk --location=US logs"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat $workdir/create_batch_sinks.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98057477-2da0-451f-a5d8-4b6cd2e77e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash $workdir/create_batch_sinks.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ccdad6-c06e-42e7-b693-145e603562a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b27ee21e-e56e-4066-9431-cf61cbf094e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT_ID\"]=PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147f4e28-97c9-4493-994a-6037f554d717",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Run the pipelines\n",
    "python3 lab1_titanic_pipeline.py \\\n",
    "  --project=$PROJECT_ID \\\n",
    "  --region=us-central1 \\\n",
    "  --stagingLocation=gs://$PROJECT_ID/staging/ \\\n",
    "  --tempLocation=gs://$PROJECT_ID/temp/ \\\n",
    "  --runner=DataflowRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aba5c0a-9ed4-452e-bc42-7a479d9cf9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
