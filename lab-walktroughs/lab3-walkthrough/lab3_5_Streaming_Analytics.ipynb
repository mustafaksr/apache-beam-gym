{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51795572-bd93-49f5-ab74-239a230f3771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lab3_streaming_minute_traffic_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lab3_streaming_minute_traffic_pipeline.py\n",
    "import argparse\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import typing\n",
    "from datetime import datetime\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.transforms.combiners import CountCombineFn \n",
    "from apache_beam.runners import DataflowRunner, DirectRunner\n",
    "\n",
    "# ### functions and classes\n",
    "\n",
    "class CommonLog(typing.NamedTuple):\n",
    "    ip: str\n",
    "    user_id: str\n",
    "    lat: float\n",
    "    lng: float\n",
    "    timestamp: str\n",
    "    http_request: str\n",
    "    http_response: int\n",
    "    num_bytes: int\n",
    "    user_agent: str\n",
    "\n",
    "beam.coders.registry.register_coder(CommonLog, beam.coders.RowCoder)\n",
    "\n",
    "def parse_json(element):\n",
    "    row = json.loads(element.decode('utf-8'))\n",
    "    return CommonLog(**row)\n",
    "\n",
    "def add_processing_timestamp(element):\n",
    "    row = element._asdict()\n",
    "    row['event_timestamp'] = row.pop('timestamp')\n",
    "    row['processing_timestamp'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return row\n",
    "\n",
    "class GetTimestampFn(beam.DoFn):\n",
    "    def process(self, element, window=beam.DoFn.WindowParam):\n",
    "        window_start = window.start.to_utc_datetime().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        output = {'page_views': element, 'timestamp': window_start}\n",
    "        yield output\n",
    "\n",
    "# ### main\n",
    "\n",
    "def run():\n",
    "    # Command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Load from Json from Pub/Sub into BigQuery')\n",
    "    parser.add_argument('--project',required=True, help='Specify Google Cloud project')\n",
    "    parser.add_argument('--region', required=True, help='Specify Google Cloud region')\n",
    "    parser.add_argument('--staging_location', required=True, help='Specify Cloud Storage bucket for staging')\n",
    "    parser.add_argument('--temp_location', required=True, help='Specify Cloud Storage bucket for temp')\n",
    "    parser.add_argument('--runner', required=True, help='Specify Apache Beam Runner')\n",
    "    parser.add_argument('--input_topic', required=True, help='Input Pub/Sub Topic')\n",
    "    parser.add_argument('--agg_table_name', required=True, help='BigQuery table name for aggregate results')\n",
    "    parser.add_argument('--raw_table_name', required=True, help='BigQuery table name for raw inputs')\n",
    "    parser.add_argument('--window_duration', required=True, help='Window duration')\n",
    "\n",
    "    opts = parser.parse_args()\n",
    "\n",
    "    # Setting up the Beam pipeline options\n",
    "    options = PipelineOptions(save_main_session=True, streaming=True)\n",
    "    options.view_as(GoogleCloudOptions).project = opts.project\n",
    "    options.view_as(GoogleCloudOptions).region = opts.region\n",
    "    options.view_as(GoogleCloudOptions).staging_location = opts.staging_location\n",
    "    options.view_as(GoogleCloudOptions).temp_location = opts.temp_location\n",
    "    options.view_as(GoogleCloudOptions).job_name = '{0}{1}'.format('streaming-minute-traffic-pipeline-',time.time_ns())\n",
    "    options.view_as(StandardOptions).runner = opts.runner\n",
    "\n",
    "    input_topic = opts.input_topic\n",
    "    raw_table_name = opts.raw_table_name\n",
    "    agg_table_name = opts.agg_table_name\n",
    "    window_duration = opts.window_duration\n",
    "\n",
    "    # Table schema for BigQuery\n",
    "    agg_table_schema = {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"page_views\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"timestamp\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    raw_table_schema = {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"ip\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"user_id\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"user_agent\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"lat\",\n",
    "                \"type\": \"FLOAT\",\n",
    "                \"mode\": \"NULLABLE\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"lng\",\n",
    "                \"type\": \"FLOAT\",\n",
    "                \"mode\": \"NULLABLE\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"event_timestamp\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"processing_timestamp\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"http_request\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"http_response\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"num_bytes\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create the pipeline\n",
    "    p = beam.Pipeline(options=options)\n",
    "\n",
    "\n",
    "\n",
    "    parsed_msgs = (p | 'ReadFromPubSub' >> beam.io.ReadFromPubSub(input_topic)\n",
    "                     | 'ParseJson' >> beam.Map(parse_json).with_output_types(CommonLog))\n",
    "\n",
    "    (parsed_msgs\n",
    "        | \"AddProcessingTimestamp\" >> beam.Map(add_processing_timestamp)\n",
    "        | 'WriteRawToBQ' >> beam.io.WriteToBigQuery(\n",
    "            raw_table_name,\n",
    "            schema=raw_table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "            )\n",
    "        )\n",
    "\n",
    "    (parsed_msgs\n",
    "        | \"WindowByMinute\" >> beam.WindowInto(beam.window.FixedWindows(60))\n",
    "        | \"CountPerMinute\" >> beam.CombineGlobally(CountCombineFn()).without_defaults()\n",
    "        | \"AddWindowTimestamp\" >> beam.ParDo(GetTimestampFn())\n",
    "        | 'WriteAggToBQ' >> beam.io.WriteToBigQuery(\n",
    "            agg_table_name,\n",
    "            schema=agg_table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "            )\n",
    "    )\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Building pipeline ...\")\n",
    "\n",
    "    p.run().wait_until_finish()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557678b7-04b2-49fd-96a1-4a2e26214e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e49f02fd-3b01-480f-91c0-618b38580570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a62ffb-9873-4506-9a0a-131ebe4ae857",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff26b3d6-6b4d-47db-8fd5-1c31d3a8e2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"workdir\"]=\"/path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fc5830b-cee6-476c-a18f-f5c5ed2125a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/#!/usr/bin/env bash\n",
      "echo \"Creating pipeline sinks\"\n",
      "\n",
      "PROJECT_ID=$(gcloud config get-value project)\n",
      "\n",
      "# GCS buckets\n",
      "#TODO: Add try/catch for the first bucket since qwiklabs\n",
      "gsutil mb -l US gs://$PROJECT_ID\n",
      "gsutil mb -l US -c \"COLDLINE\" gs://$PROJECT_ID-coldline\n",
      "\n",
      "# BiqQuery Dataset\n",
      "bq mk --location=US logs\n",
      "\n",
      "# PubSub Topic\n",
      "gcloud pubsub topics create my_topic"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat $workdir/create_streaming_sinks.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842284eb-bc93-44f6-bbd5-e98880d3a047",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash $workdir/create_streaming_sinks.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377cc245-00a0-4c07-a0cc-7396e50dc89c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03373cca-3915-49cc-8a06-a34f6e91eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT_ID\"]=PROJECT_ID\n",
    "os.environ[\"REGION\"]='us-central1'\n",
    "os.environ[\"BUCKET\"]=\"gs://\"+PROJECT_ID\n",
    "os.environ[\"PIPELINE_FOLDER\"]=\"gs://\"+PROJECT_ID\n",
    "os.environ[\"INPUT_PATH\"]=\"gs://\"+PROJECT_ID+\"/events.json\"\n",
    "os.environ[\"TABLE_NAME\"]=PROJECT_ID+\":logs.minute_traffic\"\n",
    "os.environ[\"PUBSUB_TOPIC\"]=f\"projects/{PROJECT_ID}/topics/my_topic\"\n",
    "os.environ[\"WINDOW_DURATION\"]=60\n",
    "os.environ[\"AGGREGATE_TABLE_NAME\"]=f\"{PROJECT_ID}:logs.windowed_traffic\"\n",
    "os.environ[\"RAW_TABLE_NAME\"]=f\"{PROJECT_ID}:logs.raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955c4b5e-47f1-4949-aa8a-d10da8b78b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# export PROJECT_ID=$(gcloud config get-value project)\n",
    "# export REGION='us-central1'\n",
    "# export BUCKET=gs://${PROJECT_ID}\n",
    "# export PIPELINE_FOLDER=${BUCKET}\n",
    "# export RUNNER=DataflowRunner\n",
    "# export PUBSUB_TOPIC=projects/${PROJECT_ID}/topics/my_topic\n",
    "# export WINDOW_DURATION=60\n",
    "# export AGGREGATE_TABLE_NAME=${PROJECT_ID}:logs.windowed_traffic\n",
    "# export RAW_TABLE_NAME=${PROJECT_ID}:logs.raw\n",
    "python3 lab3_streaming_minute_traffic_pipeline.py \\\n",
    "--project=${PROJECT_ID} \\\n",
    "--region=${REGION} \\\n",
    "--staging_location=${PIPELINE_FOLDER}/staging \\\n",
    "--temp_location=${PIPELINE_FOLDER}/temp \\\n",
    "--runner=${RUNNER} \\\n",
    "--input_topic=${PUBSUB_TOPIC} \\\n",
    "--window_duration=${WINDOW_DURATION} \\\n",
    "--agg_table_name=${AGGREGATE_TABLE_NAME} \\\n",
    "--raw_table_name=${RAW_TABLE_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60992dc9-5b94-4f25-b575-27b246f9c41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/#!/usr/bin/env bash\n",
      "echo \"Installing packages\"\n",
      "# Install modules\n",
      "sh ./install_packages.sh\n",
      "\n",
      "echo \"Generating synthetic users\"\n",
      "# Generate 10 fake web site users\n",
      "python3 user_generator.py --n=10\n",
      "\n",
      "echo \"Generating synthetic events\"\n",
      "use_lag=$1\n",
      "\n",
      "if [ \"$use_lag\" = true ] ; then\n",
      "    echo \"Using lag\"\n",
      "    python3 streaming_event_generator.py --project_id=$(gcloud config get-value project) -t=my_topic\n",
      "else\n",
      "    echo \"Not using lag\"\n",
      "    python3 streaming_event_generator.py --project_id=$(gcloud config get-value project) -t=my_topic -off=1. -on=0. -l=0\n",
      "fi"
     ]
    }
   ],
   "source": [
    "cat $workdir/generate_streaming_events.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8d90f9-6e49-490d-99cf-efb037fa3c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "bash $workdir/generate_streaming_events.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de097807-4eb6-452c-9bcd-6fd83604ea23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%bigquery` not found.\n"
     ]
    }
   ],
   "source": [
    "%%bigquery\n",
    "SELECT timestamp, page_views\n",
    "FROM `logs.windowed_traffic`\n",
    "ORDER BY timestamp ASC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5933318f-18d7-4cad-bef0-053be5a14a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "SELECT\n",
    "  UNIX_MILLIS(TIMESTAMP(event_timestamp)) - min_millis.min_event_millis AS event_millis,\n",
    "  UNIX_MILLIS(TIMESTAMP(processing_timestamp)) - min_millis.min_event_millis AS processing_millis,\n",
    "  user_id,\n",
    "  -- added as unique label so we see all the points\n",
    "  CAST(UNIX_MILLIS(TIMESTAMP(event_timestamp)) - min_millis.min_event_millis AS STRING) AS label\n",
    "FROM\n",
    "  `logs.raw`\n",
    "CROSS JOIN (\n",
    "  SELECT\n",
    "    MIN(UNIX_MILLIS(TIMESTAMP(event_timestamp))) AS min_event_millis\n",
    "  FROM\n",
    "    `logs.raw`) min_millis\n",
    "WHERE\n",
    "  event_timestamp IS NOT NULL\n",
    "ORDER BY\n",
    "  event_millis ASC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7ecc80-858b-42db-a66f-9b3684a77b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "bash $workdir/generate_streaming_events.sh true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca82acc9-a5ac-46cf-b62b-8f37a12a6557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
